{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e14d0e",
   "metadata": {},
   "source": [
    "# 1. Original Dataset (1min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe627d",
   "metadata": {},
   "source": [
    "## EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad459765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EEGProcessor:\n",
    "     \n",
    "    # time_interval : Unified seconds with Fitbit data (ex. 10secs) \n",
    "    # remove_time_in_group : the criteria of processing error values in each time_interval group (ex. 7secs)\n",
    "    \n",
    "    def __init__(self, file_path, time_interval, remove_time_in_group):\n",
    "        self.time_interval = time_interval\n",
    "        self.remove_time_in_group = remove_time_in_group\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        self.EEG_report = pd.read_csv(file_path)\n",
    "\n",
    "    # List to dataframe (ex. brain waves)\n",
    "    def parse_raw_data(self, dataframe, col_name):\n",
    "        col_str = dataframe.iloc[0][col_name]\n",
    "        col_str = col_str.strip('[]')\n",
    "        col_list = [float(val) for val in col_str.split(',')]  # cause list is divided by comma\n",
    "        col_data = pd.DataFrame({col_name: col_list})\n",
    "        return col_data\n",
    "\n",
    "    # Experiment time calculating function\n",
    "    def time_difference(self, dataframe, start_time_col, finish_time_col):\n",
    "        start_time = datetime.strptime(dataframe.iloc[0][start_time_col], '%Y-%m-%d %H:%M:%S')\n",
    "        finish_time = datetime.strptime(dataframe.iloc[0][finish_time_col], '%Y-%m-%d %0H:%M:%S')\n",
    "\n",
    "        # time difference between two datatime objects\n",
    "        time_difference = (finish_time - start_time).total_seconds()\n",
    "        return time_difference\n",
    "    \n",
    "    # Comparing the experimental initial recognition error period and delete the part to be deleted\n",
    "    def count_initial_same_values(self, series):\n",
    "        initial_value = series.iloc[0]\n",
    "        count = 0\n",
    "        for value in series:\n",
    "            if value == initial_value:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        return count\n",
    "    \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_start_time_trash_sec(self, start_time):\n",
    "        # Plus 1 min and delete second in input time\n",
    "        rounded_time = start_time + timedelta(minutes=1) - timedelta(seconds=start_time.second)\n",
    "        time_difference = (rounded_time - start_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        # use the seconds over remove_time_in_group seconds\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "        \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_finish_time_trash_sec(self, finish_time):\n",
    "        # delete second in input time\n",
    "        rounded_time = finish_time - timedelta(seconds=finish_time.second)\n",
    "        time_difference = (finish_time - rounded_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "\n",
    "    # Rounding time to nearest time which can divided by time interval\n",
    "    def nearest_time_rounding(self, time):\n",
    "        seconds = time.second\n",
    "        # For example, find nearest value in 0, 10, 20, 30, 40, 50sec\n",
    "        time_points = [time_point for time_point in range(0,60, self.time_interval)]\n",
    "        nearest = min(time_points, key=lambda time_point: abs(time_point - seconds))\n",
    "        \n",
    "        if nearest == time_points[-1] and seconds >= (time_points[-1] + self.remove_time_in_group):\n",
    "            rounded_time = time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n",
    "        else:\n",
    "            rounded_time = time.replace(second=nearest, microsecond=0)\n",
    "\n",
    "        return rounded_time\n",
    "\n",
    "    # Make the same end time\n",
    "    def align_end_time(self, dataframe_1, dataframe_2):\n",
    "        if dataframe_1.index[-1] > dataframe_2.index[-1]:\n",
    "            dataframe_1 = dataframe_1[dataframe_1.index <= dataframe_2.index[-1]]\n",
    "\n",
    "        elif dataframe_1.index[-1] < dataframe_2.index[-1]:\n",
    "            dataframe_2 = dataframe_2[dataframe_2.index <= dataframe_1.index[-1]]\n",
    "\n",
    "        else: \n",
    "            pass # if two dataframe's endtime is same\n",
    "\n",
    "        return dataframe_1, dataframe_2\n",
    "\n",
    "    # Adjust start time and end time processing\n",
    "    # start time processing -> process_type : 0 , finish time processing -> process_type : -1\n",
    "    # start time processing -> process_start_time_trash_sec func , finish time processing -> process_finish_time_trash_sec func    \n",
    "    def adjust_time_index(self, process_type, dataframe, func):\n",
    "        remainder = func(dataframe.index[process_type])\n",
    "        \n",
    "        # the last data only shows one original data, so processing this problem\n",
    "        one_sec = timedelta(seconds=1)\n",
    "\n",
    "        if remainder == False:\n",
    "            # change time to nearest (Start time processing)\n",
    "            if process_type == 0 :\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type])\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "            \n",
    "            # change time to nearest (Finish time processing)\n",
    "            else:\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type]) - one_sec\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "        \n",
    "        # if remainder is under remove_time_in_group, just remove\n",
    "        else:\n",
    "            cutting_time = timedelta(seconds=remainder)\n",
    "            # Start time processing\n",
    "            if process_type == 0:\n",
    "                dataframe = dataframe[dataframe.index >= dataframe.index[process_type] + cutting_time]\n",
    "            \n",
    "            # Finish time processing\n",
    "            # make the seconds like 9, 19, 29...\n",
    "            else:\n",
    "                dataframe = dataframe[dataframe.index <= dataframe.index[-1] - cutting_time - one_sec]\n",
    "\n",
    "        return dataframe\n",
    "    \n",
    "    # Removing error values in group (brain waves and attention score)\n",
    "    def check_invalid_values(self, group):\n",
    "        # find error data length in brain wave\n",
    "        alpha_invalid_series = group['α_wave_raw_data'].diff().eq(0)\n",
    "        alpha_invalid_timestamps = group.index[alpha_invalid_series].tolist()\n",
    "\n",
    "        # find error data length in attention_raw_data\n",
    "        attention_invalid_series = group['attention_raw_data'] == 0\n",
    "        attention_invalid_timestamps = group.index[attention_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        alpha_invalid = has_long_invalid_duration(alpha_invalid_timestamps)\n",
    "        attention_invalid = has_long_invalid_duration(attention_invalid_timestamps)\n",
    "\n",
    "        if alpha_invalid or attention_invalid:\n",
    "            return group.mean()\n",
    "#             return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            return group.mean()\n",
    "            # calculate average except error value\n",
    "#             valid_conditions = (\n",
    "#                 (group['α_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['β_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['θ_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['δ_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['γ_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['attention_raw_data'] != 0)\n",
    "#             )\n",
    "#             return group[valid_conditions].mean()\n",
    "\n",
    "    # Removing error values in group (hr)\n",
    "    def check_invalid_values_other(self, group):\n",
    "        # find error data length in hr\n",
    "        hr_invalid_series = group['hr_raw_data'] == 0\n",
    "        hr_invalid_timestamps = group.index[hr_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        hr_invalid = has_long_invalid_duration(hr_invalid_timestamps)\n",
    "\n",
    "        if hr_invalid:\n",
    "            return group.mean()\n",
    "#             return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "#             group = group[(group['hr_raw_data'] != 0)]\n",
    "            return group.mean()\n",
    "    \n",
    "    # Process EEG data\n",
    "    def process_eeg_data(self, experiment_id):\n",
    "        if experiment_id not in self.EEG_report.index:\n",
    "            return None\n",
    "\n",
    "        # all experiments in one df\n",
    "        EEG_report_sample = self.EEG_report.loc[[experiment_id],:]\n",
    "\n",
    "        # one dataframe for one column\n",
    "        cols = ['α_wave_raw_data', 'β_wave_raw_data', 'θ_wave_raw_data', 'δ_wave_raw_data', 'γ_wave_raw_data', 'attention_raw_data', 'hrv_raw_data', 'hr_raw_data', 'coherence_flag_raw_data']\n",
    "        parsed_dfs = [self.parse_raw_data(EEG_report_sample, col) for col in cols]\n",
    "\n",
    "        # calculate two interval second because there's two type of time interval in EEG data\n",
    "        interval_sec = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[0])\n",
    "        interval_sec_other = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[6])\n",
    "\n",
    "        # make two merged dataframe\n",
    "        merged_df = parsed_dfs[0].join(parsed_dfs[1:6])\n",
    "        merged_df_other = parsed_dfs[6].join(parsed_dfs[7:])\n",
    "\n",
    "        # experiment start time\n",
    "        start_time = datetime.strptime(EEG_report_sample.iloc[0]['meditation_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # change index to time index based on interval second\n",
    "        interval_sec, interval_sec_other = timedelta(seconds=round(interval_sec,2)), timedelta(seconds=round(interval_sec_other,2))\n",
    "        merged_df['time'] = [start_time + time * interval_sec for time in range(len(merged_df))]\n",
    "        merged_df_other['time'] = [start_time + time * interval_sec_other for time in range(len(merged_df_other))]\n",
    "        merged_df, merged_df_other = merged_df.set_index('time'), merged_df_other.set_index('time')\n",
    "        \n",
    "        # comparing the inital experiment error time\n",
    "#         counts = [self.count_initial_same_values(merged_df[col]) for col in cols[:6]] + [self.count_initial_same_values(merged_df_other['hr_raw_data'])]\n",
    "#         initial_error_times = [counts[error] * interval_sec.total_seconds() if error != 6 else counts[error] * interval_sec_other.total_seconds() for error in range(7)]\n",
    "#         initial_error_time = timedelta(seconds=max(initial_error_times))\n",
    "\n",
    "        # dataset start time\n",
    "#         real_start_time = start_time + initial_error_time\n",
    "        real_start_time = start_time\n",
    "        merged_df, merged_df_other = merged_df[merged_df.index > real_start_time], merged_df_other[merged_df_other.index > real_start_time]\n",
    "        merged_df.index, merged_df_other.index = merged_df.index.round('S'), merged_df_other.index.round('S')\n",
    "\n",
    "        # make the experiment end time same\n",
    "        merged_df, merged_df_other = self.align_end_time(merged_df, merged_df_other)\n",
    "\n",
    "        # start time process -> i : 0 , finish time process -> i : -1\n",
    "        # start time process -> process_start_time_trash_sec func , finish time process -> process_finish_time_trash_sec func\n",
    "        merged_df = self.adjust_time_index(0, merged_df, self.process_start_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(0, merged_df_other, self.process_start_time_trash_sec)\n",
    "        merged_df = self.adjust_time_index(-1, merged_df, self.process_finish_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(-1, merged_df_other, self.process_finish_time_trash_sec)\n",
    "\n",
    "        # grouping\n",
    "        grouped = merged_df.groupby(merged_df.index.floor(self.time_interval_str))\n",
    "        grouped_other = merged_df_other.groupby(merged_df_other.index.floor(self.time_interval_str))\n",
    "\n",
    "        result = grouped.apply(self.check_invalid_values)\n",
    "        result_other = grouped_other.apply(self.check_invalid_values_other)\n",
    "\n",
    "        # final EEG dataset including β/θ SP ratio\n",
    "        EEG_data_per_time_interval = result.merge(result_other, left_index=True, right_index=True)\n",
    "        EEG_data_per_time_interval['β/θ SP'] = EEG_data_per_time_interval['β_wave_raw_data'] / EEG_data_per_time_interval['θ_wave_raw_data']\n",
    "        \n",
    "        EEG_data_per_time_interval = EEG_data_per_time_interval.rename(columns={\n",
    "            'α_wave_raw_data':'alpha_wave',\n",
    "            'β_wave_raw_data':'beta_wave',\n",
    "            'θ_wave_raw_data':'theta_wave',\n",
    "            'δ_wave_raw_data':'delta_wave',\n",
    "            'γ_wave_raw_data':'gamma_wave',\n",
    "            'attention_raw_data' : 'attention',\n",
    "            'hrv_raw_data' : 'hrv',\n",
    "            'hr_raw_data' : 'hr',\n",
    "            'coherence_flag_raw_data' : 'coherence',\n",
    "            'β/θ SP' : 'SP ratio'\n",
    "        })\n",
    "\n",
    "        return EEG_data_per_time_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec4611",
   "metadata": {},
   "source": [
    "## Fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1dcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "class FitbitProcessor:\n",
    "    '''\n",
    "    time_interval : Unified seconds with EEG dataset (ex.10secs) \n",
    "    BM_sum_minutes\n",
    "    Body Movement feature was meaningless because the experiment was conducted usually while sitting on the chair.\n",
    "    So, created a new body movement feature as accumulated Body Movement value from previous time.\n",
    "    And, the previous time is the BM_sum_minutes variable. (ex. 3 hours)\n",
    "    '''\n",
    "    def __init__(self, folder_path, time_interval, BM_sum_minutes):\n",
    "        self.folder_path = folder_path\n",
    "        self.BM_sum_minutes = BM_sum_minutes\n",
    "        self.BM_sum_minutes_str = f'{BM_sum_minutes}T'\n",
    "        self.time_interval = time_interval\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        folder_patterns = [\n",
    "            \"Active Zone Minutes (AZM)/*\",\n",
    "            \"Sleep Score/*\",\n",
    "            \"Stress Journal/CEDA*\",\n",
    "            \"Temperature/Wrist Temperature - *\"\n",
    "        ]\n",
    "        self.things_path = [glob.glob(f\"{folder_path}/{pattern}\") for pattern in folder_patterns]\n",
    "        self.things_path = [item for sublist in self.things_path for item in sublist]\n",
    "        self.wt_count = len(glob.glob(f\"{folder_path}/Temperature/Wrist Temperature - *\"))\n",
    "        self.azm_count = len(glob.glob(f\"{folder_path}/Active Zone Minutes (AZM)/*\"))\n",
    "        self.sleep_count = len(glob.glob(f\"{folder_path}/Sleep Score/*\"))\n",
    "        self.eda_count = len(glob.glob(f\"{folder_path}/Stress Journal/CEDA*\"))\n",
    "        # original fitbit dataset's time interval is 1 min\n",
    "        self.original_interval = 60\n",
    "        self.num_timestamps = self.original_interval // self.time_interval\n",
    "        self.half_point = self.num_timestamps // 2\n",
    "    \n",
    "    def read_filtered_csv(self, path, columns):\n",
    "        name = pd.read_csv(path)\n",
    "        name = name[columns]\n",
    "        name[columns[0]] = pd.to_datetime(name[columns[0]])\n",
    "        return name\n",
    "    \n",
    "    def round_seconds(self, obj):\n",
    "        if obj.second % self.time_interval == 0:\n",
    "            return obj\n",
    "        else:\n",
    "            return obj - timedelta(seconds=obj.second % self.time_interval)\n",
    "    \n",
    "    def round_zero(self, datetime_obj):\n",
    "        datetime_obj = datetime_obj.replace(second=0)\n",
    "        return datetime_obj\n",
    "    \n",
    "    # Process whole fitbit data\n",
    "    def process_fitbit_data(self):\n",
    "        AZM_col = ['date_time', 'total_minutes']\n",
    "        sleep_col = ['timestamp', 'deep_sleep_in_minutes']\n",
    "        stress_col = ['timestamp', 'eda_level_real']\n",
    "        temp_col = ['recorded_time', 'temperature']\n",
    "        \n",
    "        # merge all features\n",
    "        things_col = [AZM_col] * self.azm_count + [sleep_col] * self.sleep_count + [stress_col] * self.eda_count + [temp_col] * self.wt_count\n",
    "        things = [self.read_filtered_csv(path, col) for path, col in zip(self.things_path, things_col)]\n",
    "\n",
    "        # if there's no wrist temperature\n",
    "        if self.wt_count == 0:\n",
    "            # if there's no eda data\n",
    "            # there was no eda in two subjects' fitbit data\n",
    "            if self.eda_count == 0:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep])\n",
    "                \n",
    "                # make final dataframe\n",
    "                df = self.create_final_df([azm, sleep], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                df = df.assign(eda=np.nan)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            else:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda])\n",
    "\n",
    "                df = self.create_final_df([azm, sleep, eda], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                return df\n",
    "            \n",
    "        # if there's no Active Zone Minutes data\n",
    "        elif self.azm_count == 0:\n",
    "            sleep = self.process_sleep(things[self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(BM=np.nan)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # if there's no sleep data\n",
    "        elif self.sleep_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(sleep=np.nan)\n",
    "\n",
    "            return df            \n",
    "        \n",
    "        # if there's no eda data\n",
    "        elif self.eda_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(eda=np.nan)\n",
    "\n",
    "            return df\n",
    "            \n",
    "        # if there's no error in data file\n",
    "        else:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "\n",
    "            return df\n",
    "    \n",
    "    def process_azm(self, azm):\n",
    "\n",
    "        azm = pd.concat(azm, axis=0)\n",
    "        azm = azm.rename(columns={'date_time':'timestamp', 'total_minutes':'BM'})\n",
    "        time_list = []\n",
    "        body_movement = []\n",
    "\n",
    "        for time in azm['timestamp']:\n",
    "            for _ in range(self.num_timestamps):\n",
    "                time_list.append(time)\n",
    "                \n",
    "        for bm in azm['BM']:\n",
    "            for _ in range(self.num_timestamps):\n",
    "                body_movement.append(bm)\n",
    "\n",
    "        azm_list = {'timestamp': time_list, 'BM': body_movement}\n",
    "        azm_final = pd.DataFrame(azm_list)\n",
    "\n",
    "#         azm_final['timestamp'] = pd.to_datetime(azm_final['timestamp'])\n",
    "#         azm_final.set_index('timestamp', inplace=True)\n",
    "#         azm_final = azm_final.resample(self.time_interval_str).asfreq().fillna(0)\n",
    "#         azm_final['new_BM'] = azm_final['BM'].rolling(self.BM_sum_minutes_str, closed='right').sum()\n",
    "#         azm_final = azm_final.drop(['BM'], axis=1)\n",
    "#         azm_final = azm_final.rename(columns={'new_BM':'BM'})\n",
    "#         azm_final = azm_final.astype({'BM':'int'})\n",
    "#         azm_final.reset_index(inplace=True)\n",
    "        \n",
    "        return azm_final\n",
    "    \n",
    "    # Process sleep data (Deep sleep in minutes)\n",
    "    def process_sleep(self, sleep):\n",
    "        sleep = pd.concat(sleep, axis=0)\n",
    "        sleep['timestamp'] = [\n",
    "                self.round_zero(datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S')) for time in sleep['timestamp']\n",
    "            ]\n",
    "            \n",
    "        sleep = sleep.rename(columns={'deep_sleep_in_minutes':'sleep'})\n",
    "        return sleep\n",
    "    \n",
    "    # Process eda data\n",
    "    def process_eda(self, eda):\n",
    "        eda = pd.concat(eda, axis=0)\n",
    "        eda['timestamp'] = [\n",
    "            self.round_seconds(\n",
    "                datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=4)\n",
    "            ) for time in eda['timestamp']]\n",
    "\n",
    "        # EDA Bilinear Interpolation\n",
    "        time_list = []\n",
    "        eda_list = []\n",
    "        \n",
    "        for time in range(1, len(eda['timestamp']) - 1):\n",
    "            start_timestamp = eda.iloc[time,0]\n",
    "            for num in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "                value = eda.iloc[time,1]\n",
    "                eda_list.append(round(value, 2))\n",
    "\n",
    "        eda_list = {'timestamp': time_list, 'eda': eda_list}\n",
    "        eda_final = pd.DataFrame(eda_list)\n",
    "\n",
    "        return eda_final\n",
    "    \n",
    "    # Process temperature data\n",
    "    def process_temperature(self, temperature):\n",
    "        #Temperature\n",
    "        temp = pd.concat(temperature, axis=0)\n",
    "        temp = temp.rename(columns={'recorded_time':'timestamp'})\n",
    "\n",
    "        # Temperature Bilinear Interpolation\n",
    "        time_list = []\n",
    "        temp_list = []\n",
    "\n",
    "        for time in range(1, len(temp['timestamp']) - 1):\n",
    "            start_timestamp = temp.iloc[time,0]\n",
    "            for num in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "                value = temp.iloc[time,1]\n",
    "                temp_list.append(round(value, 6))\n",
    "\n",
    "        temp_list = {'timestamp': time_list, 'temperature': temp_list}\n",
    "        temp_final = pd.DataFrame(temp_list)\n",
    "\n",
    "        return temp_final\n",
    "    \n",
    "    # find minimum and maximum time of whole feature\n",
    "    def find_time_bounds(self, dataframes):\n",
    "        min_times = []\n",
    "        max_times = []\n",
    "        \n",
    "        for df in dataframes:\n",
    "            if not df.empty:\n",
    "                min_times.append(df['timestamp'].min())\n",
    "                max_times.append(df['timestamp'].max())\n",
    "                \n",
    "        if not min_times or not max_times:\n",
    "            Min_Time = pd.Timestamp.now(tz='UTC')\n",
    "            Max_Time = pd.Timestamp.now(tz='UTC')\n",
    "        else:\n",
    "            Min_Time = min(min_times)\n",
    "            Max_Time = max(max_times)\n",
    "\n",
    "        return Min_Time, Max_Time\n",
    "\n",
    "    # create dataframe from Min_time to Max_time\n",
    "    def create_final_df(self, datasets, Min_Time, Max_Time):\n",
    "        fitbit = pd.date_range(start=Min_Time, end=Max_Time, freq=self.time_interval_str, name='timestamp')\n",
    "        fitbit = pd.DataFrame(fitbit)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            fitbit = pd.merge(fitbit, dataset, how='outer', on='timestamp')\n",
    "            \n",
    "        fitbit['BM'] = fitbit['BM'].fillna(0)\n",
    "        fitbit['sleep'] = fitbit['sleep'].fillna(method='ffill')\n",
    "\n",
    "        return fitbit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d8a5d",
   "metadata": {},
   "source": [
    "### Merging EEG and Fitbit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45002ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger(EEGProcessor, FitbitProcessor):\n",
    "    def __init__(self, eeg_filepath, fitbit_folderpath, time_interval=60, eeg_remove_time_in_group=45, BM_sum_minutes=180):\n",
    "        # Initialize by calling parent class constructor\n",
    "        EEGProcessor.__init__(self, eeg_filepath, time_interval, eeg_remove_time_in_group)\n",
    "        FitbitProcessor.__init__(self, fitbit_folderpath, time_interval, BM_sum_minutes)\n",
    "        \n",
    "        self.eeg_filepath = eeg_filepath\n",
    "    # Merge EEG and Fitbit data\n",
    "    def merge_data(self):\n",
    "        # processing EEG data\n",
    "        eeg_data = pd.read_csv(self.eeg_filepath)\n",
    "        result_dfs = []\n",
    "        \n",
    "        # For all experiments in the eeg data csv file\n",
    "        for exp_id in range(3, len(eeg_data)):\n",
    "            processed_data = self.process_eeg_data(exp_id)\n",
    "            if processed_data is not None:\n",
    "                result_dfs.append(processed_data)\n",
    "                \n",
    "        if result_dfs:\n",
    "            combined_eeg = pd.concat(result_dfs)\n",
    "            combined_eeg.index = pd.to_datetime(combined_eeg.index)\n",
    "        \n",
    "        # processing Fitbit data\n",
    "        fitbit_data = self.process_fitbit_data()\n",
    "        fitbit_data.index = pd.to_datetime(fitbit_data.index)\n",
    "\n",
    "        # merging two dataframes\n",
    "        if 'combined_eeg' in locals() and not fitbit_data.empty:\n",
    "            merged_df = combined_eeg.merge(fitbit_data, left_index=True, right_index=True, how='left')\n",
    "            return merged_df\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035429d",
   "metadata": {},
   "source": [
    "### JM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ef645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_jm.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_JM\")\n",
    "\n",
    "final_jm = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b9eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_jm.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e1392",
   "metadata": {},
   "source": [
    "### YH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc341b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_yh.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_YH\")\n",
    "\n",
    "final_yh = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9ef30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_yh.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c063596",
   "metadata": {},
   "source": [
    "### SJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc936ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SJ\")\n",
    "\n",
    "\n",
    "final_sj = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded75a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_sj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9331e3",
   "metadata": {},
   "source": [
    "### SA - no CEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e639727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sa.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SA\")\n",
    "\n",
    "\n",
    "final_sa = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8cbc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c08ef",
   "metadata": {},
   "source": [
    "### BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1a74d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_bs.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_BS\")\n",
    "\n",
    "\n",
    "final_bs = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01850325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56e52a",
   "metadata": {},
   "source": [
    "### MJ - no CEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1c133",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_mj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_MJ\")\n",
    "\n",
    "\n",
    "final_mj = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7388c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_mj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9d31c",
   "metadata": {},
   "source": [
    "### Concat EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e50fba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([final_jm, final_sj, final_bs, final_yh, final_mj, final_sa])\n",
    "dataset = dataset.sort_index()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(r'C:\\Users\\ballj\\OneDrive\\바탕 화면\\1.1m_grouped_mean_not_removing_error_value_in_original_data_no_bm_process.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a25dc",
   "metadata": {},
   "source": [
    "# 2. Augmentation (10sec group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b2c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EEGProcessor:\n",
    "     \n",
    "    # time_interval : Unified seconds with Fitbit data (ex. 10secs) \n",
    "    # remove_time_in_group : the criteria of processing error values in each time_interval group (ex. 7secs)\n",
    "    \n",
    "    def __init__(self, file_path, time_interval, remove_time_in_group):\n",
    "        self.time_interval = time_interval\n",
    "        self.remove_time_in_group = remove_time_in_group\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        self.EEG_report = pd.read_csv(file_path)\n",
    "\n",
    "    # List to dataframe (ex. brain waves)\n",
    "    def parse_raw_data(self, dataframe, col_name):\n",
    "        col_str = dataframe.iloc[0][col_name]\n",
    "        col_str = col_str.strip('[]')\n",
    "        col_list = [float(val) for val in col_str.split(',')]  # cause list is divided by comma\n",
    "        col_data = pd.DataFrame({col_name: col_list})\n",
    "        return col_data\n",
    "\n",
    "    # Experiment time calculating function\n",
    "    def time_difference(self, dataframe, start_time_col, finish_time_col):\n",
    "        start_time = datetime.strptime(dataframe.iloc[0][start_time_col], '%Y-%m-%d %H:%M:%S')\n",
    "        finish_time = datetime.strptime(dataframe.iloc[0][finish_time_col], '%Y-%m-%d %0H:%M:%S')\n",
    "\n",
    "        # time difference between two datatime objects\n",
    "        time_difference = (finish_time - start_time).total_seconds()\n",
    "        return time_difference\n",
    "    \n",
    "    # Comparing the experimental initial recognition error period and delete the part to be deleted\n",
    "    def count_initial_same_values(self, series):\n",
    "        initial_value = series.iloc[0]\n",
    "        count = 0\n",
    "        for value in series:\n",
    "            if value == initial_value:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        return count\n",
    "    \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_start_time_trash_sec(self, start_time):\n",
    "        # Plus 1 min and delete second in input time\n",
    "        rounded_time = start_time + timedelta(minutes=1) - timedelta(seconds=start_time.second)\n",
    "        time_difference = (rounded_time - start_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        # use the seconds over remove_time_in_group seconds\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "        \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_finish_time_trash_sec(self, finish_time):\n",
    "        # delete second in input time\n",
    "        rounded_time = finish_time - timedelta(seconds=finish_time.second)\n",
    "        time_difference = (finish_time - rounded_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "\n",
    "    # Rounding time to nearest time which can divided by time interval\n",
    "    def nearest_time_rounding(self, time):\n",
    "        seconds = time.second\n",
    "        # For example, find nearest value in 0, 10, 20, 30, 40, 50sec\n",
    "        time_points = [time_point for time_point in range(0,60, self.time_interval)]\n",
    "        nearest = min(time_points, key=lambda time_point: abs(time_point - seconds))\n",
    "        \n",
    "        if nearest == time_points[-1] and seconds >= (time_points[-1] + self.remove_time_in_group):\n",
    "            rounded_time = time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n",
    "        else:\n",
    "            rounded_time = time.replace(second=nearest, microsecond=0)\n",
    "\n",
    "        return rounded_time\n",
    "\n",
    "    # Make the same end time\n",
    "    def align_end_time(self, dataframe_1, dataframe_2):\n",
    "        if dataframe_1.index[-1] > dataframe_2.index[-1]:\n",
    "            dataframe_1 = dataframe_1[dataframe_1.index <= dataframe_2.index[-1]]\n",
    "\n",
    "        elif dataframe_1.index[-1] < dataframe_2.index[-1]:\n",
    "            dataframe_2 = dataframe_2[dataframe_2.index <= dataframe_1.index[-1]]\n",
    "\n",
    "        else: \n",
    "            pass # if two dataframe's endtime is same\n",
    "\n",
    "        return dataframe_1, dataframe_2\n",
    "\n",
    "    # Adjust start time and end time processing\n",
    "    # start time processing -> process_type : 0 , finish time processing -> process_type : -1\n",
    "    # start time processing -> process_start_time_trash_sec func , finish time processing -> process_finish_time_trash_sec func    \n",
    "    def adjust_time_index(self, process_type, dataframe, func):\n",
    "        remainder = func(dataframe.index[process_type])\n",
    "        \n",
    "        # the last data only shows one original data, so processing this problem\n",
    "        one_sec = timedelta(seconds=1)\n",
    "\n",
    "        if remainder == False:\n",
    "            # change time to nearest (Start time processing)\n",
    "            if process_type == 0 :\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type])\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "            \n",
    "            # change time to nearest (Finish time processing)\n",
    "            else:\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type]) - one_sec\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "        \n",
    "        # if remainder is under remove_time_in_group, just remove\n",
    "        else:\n",
    "            cutting_time = timedelta(seconds=remainder)\n",
    "            # Start time processing\n",
    "            if process_type == 0:\n",
    "                dataframe = dataframe[dataframe.index >= dataframe.index[process_type] + cutting_time]\n",
    "            \n",
    "            # Finish time processing\n",
    "            # make the seconds like 9, 19, 29...\n",
    "            else:\n",
    "                dataframe = dataframe[dataframe.index <= dataframe.index[-1] - cutting_time - one_sec]\n",
    "\n",
    "        return dataframe\n",
    "    \n",
    "    # Removing error values in group (brain waves and attention score)\n",
    "    def check_invalid_values(self, group):\n",
    "        # find error data length in brain wave\n",
    "        alpha_invalid_series = group['α_wave_raw_data'].diff().eq(0)\n",
    "        alpha_invalid_timestamps = group.index[alpha_invalid_series].tolist()\n",
    "\n",
    "        # find error data length in attention_raw_data\n",
    "        attention_invalid_series = group['attention_raw_data'] == 0\n",
    "        attention_invalid_timestamps = group.index[attention_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        alpha_invalid = has_long_invalid_duration(alpha_invalid_timestamps)\n",
    "        attention_invalid = has_long_invalid_duration(attention_invalid_timestamps)\n",
    "\n",
    "        if alpha_invalid or attention_invalid:\n",
    "            return group.mean()\n",
    "#             return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            return group.mean()\n",
    "            # calculate average except error value\n",
    "#             valid_conditions = (\n",
    "#                 (group['α_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['β_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['θ_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['δ_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['γ_wave_raw_data'].diff() != 0) & \n",
    "#                 (group['attention_raw_data'] != 0)\n",
    "#             )\n",
    "#             return group[valid_conditions].mean()\n",
    "\n",
    "    # Removing error values in group (hr)\n",
    "    def check_invalid_values_other(self, group):\n",
    "        # find error data length in hr\n",
    "        hr_invalid_series = group['hr_raw_data'] == 0\n",
    "        hr_invalid_timestamps = group.index[hr_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        hr_invalid = has_long_invalid_duration(hr_invalid_timestamps)\n",
    "\n",
    "        if hr_invalid:\n",
    "            return group.mean()\n",
    "#             return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "#             group = group[(group['hr_raw_data'] != 0)]\n",
    "            return group.mean()\n",
    "    \n",
    "    # Process EEG data\n",
    "    def process_eeg_data(self, experiment_id):\n",
    "        if experiment_id not in self.EEG_report.index:\n",
    "            return None\n",
    "\n",
    "        # all experiments in one df\n",
    "        EEG_report_sample = self.EEG_report.loc[[experiment_id],:]\n",
    "\n",
    "        # one dataframe for one column\n",
    "        cols = ['α_wave_raw_data', 'β_wave_raw_data', 'θ_wave_raw_data', 'δ_wave_raw_data', 'γ_wave_raw_data', 'attention_raw_data', 'hrv_raw_data', 'hr_raw_data', 'coherence_flag_raw_data']\n",
    "        parsed_dfs = [self.parse_raw_data(EEG_report_sample, col) for col in cols]\n",
    "\n",
    "        # calculate two interval second because there's two type of time interval in EEG data\n",
    "        interval_sec = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[0])\n",
    "        interval_sec_other = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[6])\n",
    "\n",
    "        # make two merged dataframe\n",
    "        merged_df = parsed_dfs[0].join(parsed_dfs[1:6])\n",
    "        merged_df_other = parsed_dfs[6].join(parsed_dfs[7:])\n",
    "\n",
    "        # experiment start time\n",
    "        start_time = datetime.strptime(EEG_report_sample.iloc[0]['meditation_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # change index to time index based on interval second\n",
    "        interval_sec, interval_sec_other = timedelta(seconds=round(interval_sec,2)), timedelta(seconds=round(interval_sec_other,2))\n",
    "        merged_df['time'] = [start_time + time * interval_sec for time in range(len(merged_df))]\n",
    "        merged_df_other['time'] = [start_time + time * interval_sec_other for time in range(len(merged_df_other))]\n",
    "        merged_df, merged_df_other = merged_df.set_index('time'), merged_df_other.set_index('time')\n",
    "        \n",
    "        # comparing the inital experiment error time\n",
    "#         counts = [self.count_initial_same_values(merged_df[col]) for col in cols[:6]] + [self.count_initial_same_values(merged_df_other['hr_raw_data'])]\n",
    "#         initial_error_times = [counts[error] * interval_sec.total_seconds() if error != 6 else counts[error] * interval_sec_other.total_seconds() for error in range(7)]\n",
    "#         initial_error_time = timedelta(seconds=max(initial_error_times))\n",
    "\n",
    "        # dataset start time\n",
    "#         real_start_time = start_time + initial_error_time\n",
    "        real_start_time = start_time\n",
    "        merged_df, merged_df_other = merged_df[merged_df.index > real_start_time], merged_df_other[merged_df_other.index > real_start_time]\n",
    "        merged_df.index, merged_df_other.index = merged_df.index.round('S'), merged_df_other.index.round('S')\n",
    "\n",
    "        # make the experiment end time same\n",
    "        merged_df, merged_df_other = self.align_end_time(merged_df, merged_df_other)\n",
    "\n",
    "        # start time process -> i : 0 , finish time process -> i : -1\n",
    "        # start time process -> process_start_time_trash_sec func , finish time process -> process_finish_time_trash_sec func\n",
    "        merged_df = self.adjust_time_index(0, merged_df, self.process_start_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(0, merged_df_other, self.process_start_time_trash_sec)\n",
    "        merged_df = self.adjust_time_index(-1, merged_df, self.process_finish_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(-1, merged_df_other, self.process_finish_time_trash_sec)\n",
    "\n",
    "        # grouping\n",
    "        grouped = merged_df.groupby(merged_df.index.floor(self.time_interval_str))\n",
    "        grouped_other = merged_df_other.groupby(merged_df_other.index.floor(self.time_interval_str))\n",
    "\n",
    "        result = grouped.apply(self.check_invalid_values)\n",
    "        result_other = grouped_other.apply(self.check_invalid_values_other)\n",
    "\n",
    "        # final EEG dataset including β/θ SP ratio\n",
    "        EEG_data_per_time_interval = result.merge(result_other, left_index=True, right_index=True)\n",
    "        EEG_data_per_time_interval['β/θ SP'] = EEG_data_per_time_interval['β_wave_raw_data'] / EEG_data_per_time_interval['θ_wave_raw_data']\n",
    "        \n",
    "        EEG_data_per_time_interval = EEG_data_per_time_interval.rename(columns={\n",
    "            'α_wave_raw_data':'alpha_wave',\n",
    "            'β_wave_raw_data':'beta_wave',\n",
    "            'θ_wave_raw_data':'theta_wave',\n",
    "            'δ_wave_raw_data':'delta_wave',\n",
    "            'γ_wave_raw_data':'gamma_wave',\n",
    "            'attention_raw_data' : 'attention',\n",
    "            'hrv_raw_data' : 'hrv',\n",
    "            'hr_raw_data' : 'hr',\n",
    "            'coherence_flag_raw_data' : 'coherence',\n",
    "            'β/θ SP' : 'SP ratio'\n",
    "        })\n",
    "\n",
    "        return EEG_data_per_time_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15477005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "class FitbitProcessor:\n",
    "    '''\n",
    "    time_interval : Unified seconds with EEG dataset (ex.10secs) \n",
    "    BM_sum_minutes\n",
    "    Body Movement feature was meaningless because the experiment was conducted usually while sitting on the chair.\n",
    "    So, created a new body movement feature as accumulated Body Movement value from previous time.\n",
    "    And, the previous time is the BM_sum_minutes variable. (ex. 3 hours)\n",
    "    '''\n",
    "    def __init__(self, folder_path, time_interval, BM_sum_minutes):\n",
    "        self.folder_path = folder_path\n",
    "        self.BM_sum_minutes = BM_sum_minutes\n",
    "        self.BM_sum_minutes_str = f'{BM_sum_minutes}T'\n",
    "        self.time_interval = time_interval\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        folder_patterns = [\n",
    "            \"Active Zone Minutes (AZM)/*\",\n",
    "            \"Sleep Score/*\",\n",
    "            \"Stress Journal/CEDA*\",\n",
    "            \"Temperature/Wrist Temperature - *\"\n",
    "        ]\n",
    "        self.things_path = [glob.glob(f\"{folder_path}/{pattern}\") for pattern in folder_patterns]\n",
    "        self.things_path = [item for sublist in self.things_path for item in sublist]\n",
    "        self.wt_count = len(glob.glob(f\"{folder_path}/Temperature/Wrist Temperature - *\"))\n",
    "        self.azm_count = len(glob.glob(f\"{folder_path}/Active Zone Minutes (AZM)/*\"))\n",
    "        self.sleep_count = len(glob.glob(f\"{folder_path}/Sleep Score/*\"))\n",
    "        self.eda_count = len(glob.glob(f\"{folder_path}/Stress Journal/CEDA*\"))\n",
    "        # original fitbit dataset's time interval is 1 min\n",
    "        self.original_interval = 60\n",
    "        self.num_timestamps = self.original_interval // self.time_interval\n",
    "        self.half_point = self.num_timestamps // 2\n",
    "    \n",
    "    def read_filtered_csv(self, path, columns):\n",
    "        name = pd.read_csv(path)\n",
    "        name = name[columns]\n",
    "        name[columns[0]] = pd.to_datetime(name[columns[0]])\n",
    "        return name\n",
    "    \n",
    "    def round_seconds(self, obj):\n",
    "        if obj.second % self.time_interval == 0:\n",
    "            return obj\n",
    "        else:\n",
    "            return obj - timedelta(seconds=obj.second % self.time_interval)\n",
    "    \n",
    "    def round_zero(self, datetime_obj):\n",
    "        datetime_obj = datetime_obj.replace(second=0)\n",
    "        return datetime_obj\n",
    "    \n",
    "    # Process whole fitbit data\n",
    "    def process_fitbit_data(self):\n",
    "        AZM_col = ['date_time', 'total_minutes']\n",
    "        sleep_col = ['timestamp', 'deep_sleep_in_minutes']\n",
    "        stress_col = ['timestamp', 'eda_level_real']\n",
    "        temp_col = ['recorded_time', 'temperature']\n",
    "        \n",
    "        # merge all features\n",
    "        things_col = [AZM_col] * self.azm_count + [sleep_col] * self.sleep_count + [stress_col] * self.eda_count + [temp_col] * self.wt_count\n",
    "        things = [self.read_filtered_csv(path, col) for path, col in zip(self.things_path, things_col)]\n",
    "\n",
    "        # if there's no wrist temperature\n",
    "        if self.wt_count == 0:\n",
    "            # if there's no eda data\n",
    "            # there was no eda in two subjects' fitbit data\n",
    "            if self.eda_count == 0:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep])\n",
    "                \n",
    "                # make final dataframe\n",
    "                df = self.create_final_df([azm, sleep], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                df = df.assign(eda=np.nan)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            else:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda])\n",
    "\n",
    "                df = self.create_final_df([azm, sleep, eda], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                return df\n",
    "            \n",
    "        # if there's no Active Zone Minutes data\n",
    "        elif self.azm_count == 0:\n",
    "            sleep = self.process_sleep(things[self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(BM=np.nan)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # if there's no sleep data\n",
    "        elif self.sleep_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(sleep=np.nan)\n",
    "\n",
    "            return df            \n",
    "        \n",
    "        # if there's no eda data\n",
    "        elif self.eda_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(eda=np.nan)\n",
    "\n",
    "            return df\n",
    "            \n",
    "        # if there's no error in data file\n",
    "        else:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "\n",
    "            return df\n",
    "    \n",
    "    # Process Active Zone Minutes data\n",
    "    def process_azm(self, azm):\n",
    "\n",
    "        azm = pd.concat(azm, axis=0)\n",
    "        azm = azm.rename(columns={'date_time':'timestamp', 'total_minutes':'BM'})\n",
    "        time_list = []\n",
    "        body_movement = []\n",
    "\n",
    "        for time in azm['timestamp']:\n",
    "            start_timestamp = time - timedelta(seconds=(self.half_point * self.time_interval))\n",
    "\n",
    "            for number in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (number*self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "        for bm in azm['BM']:\n",
    "            for _ in range(self.num_timestamps):\n",
    "                body_movement.append(bm)\n",
    "\n",
    "        azm_list = {'timestamp': time_list, 'BM': body_movement}\n",
    "        azm_final = pd.DataFrame(azm_list)\n",
    "\n",
    "#         azm_final['timestamp'] = pd.to_datetime(azm_final['timestamp'])\n",
    "#         azm_final.set_index('timestamp', inplace=True)\n",
    "#         azm_final = azm_final.resample(self.time_interval_str).asfreq().fillna(0)\n",
    "#         azm_final['new_BM'] = azm_final['BM'].rolling(self.BM_sum_minutes_str, closed='right').sum()\n",
    "#         azm_final = azm_final.drop(['BM'], axis=1)\n",
    "#         azm_final = azm_final.rename(columns={'new_BM':'BM'})\n",
    "#         azm_final = azm_final.astype({'BM':'int'})\n",
    "#         azm_final.reset_index(inplace=True)\n",
    "        \n",
    "        return azm_final\n",
    "    \n",
    "    # Process sleep data (Deep sleep in minutes)\n",
    "    def process_sleep(self, sleep):\n",
    "        sleep = pd.concat(sleep, axis=0)\n",
    "        sleep['timestamp'] = [\n",
    "                self.round_zero(datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S')) for time in sleep['timestamp']\n",
    "            ]\n",
    "            \n",
    "        sleep = sleep.rename(columns={'deep_sleep_in_minutes':'sleep'})\n",
    "        return sleep\n",
    "    \n",
    "    # Process eda data\n",
    "    def process_eda(self, eda):\n",
    "        eda = pd.concat(eda, axis=0)\n",
    "        eda['timestamp'] = [\n",
    "            self.round_seconds(\n",
    "                datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=4)\n",
    "            ) for time in eda['timestamp']]\n",
    "\n",
    "        # EDA Bilinear Interpolation\n",
    "        time_list = []\n",
    "        eda_list = []\n",
    "\n",
    "        for time in range(1, len(eda['timestamp']) - 1):\n",
    "            start_timestamp = eda.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "\n",
    "            for num in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "                if num < self.half_point:\n",
    "                    weight = (self.half_point - num) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] - ((eda.iloc[time, 1] - eda.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                elif num == self.half_point:\n",
    "                    value = eda.iloc[time,1]\n",
    "\n",
    "                else:\n",
    "                    weight = (num - self.half_point) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] + ((eda.iloc[time + 1, 1] - eda.iloc[time, 1]) * weight)\n",
    "\n",
    "                eda_list.append(round(value, 2))\n",
    "\n",
    "        eda_list = {'timestamp': time_list, 'eda': eda_list}\n",
    "        eda_final = pd.DataFrame(eda_list)\n",
    "\n",
    "        return eda_final\n",
    "    \n",
    "    # Process temperature data\n",
    "    def process_temperature(self, temperature):\n",
    "        #Temperature\n",
    "        temp = pd.concat(temperature, axis=0)\n",
    "        temp = temp.rename(columns={'recorded_time':'timestamp'})\n",
    "\n",
    "        # Temperature Bilinear Interpolation\n",
    "        time_list = []\n",
    "        temp_list = []\n",
    "\n",
    "        for time in range(1, len(temp['timestamp']) - 1):\n",
    "            if self.half_point % 2 != 0:\n",
    "                start_timestamp = temp.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "                for num in range(self.num_timestamps):\n",
    "                    new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                    time_list.append(new_timestamp)\n",
    "\n",
    "                    if num < self.half_point:\n",
    "                        weight = (self.half_point - num) / self.num_timestamps \n",
    "                        value = temp.iloc[time, 1] - ((temp.iloc[time, 1] - temp.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                    elif num == self.half_point:\n",
    "                        value = temp.iloc[time,1]\n",
    "\n",
    "                    else:\n",
    "                        weight = (num - self.half_point) / self.num_timestamps\n",
    "                        value = temp.iloc[time, 1] + ((temp.iloc[time + 1, 1] - temp.iloc[time, 1]) * weight)\n",
    "\n",
    "                    temp_list.append(round(value, 6))\n",
    "\n",
    "        temp_list = {'timestamp': time_list, 'temperature': temp_list}\n",
    "        temp_final = pd.DataFrame(temp_list)\n",
    "\n",
    "        return temp_final\n",
    "    \n",
    "    # find minimum and maximum time of whole feature\n",
    "    def find_time_bounds(self, dataframes):\n",
    "        min_times = []\n",
    "        max_times = []\n",
    "        \n",
    "        for df in dataframes:\n",
    "            if not df.empty:\n",
    "                min_times.append(df['timestamp'].min())\n",
    "                max_times.append(df['timestamp'].max())\n",
    "                \n",
    "        if not min_times or not max_times:\n",
    "            Min_Time = pd.Timestamp.now(tz='UTC')\n",
    "            Max_Time = pd.Timestamp.now(tz='UTC')\n",
    "        else:\n",
    "            Min_Time = min(min_times)\n",
    "            Max_Time = max(max_times)\n",
    "\n",
    "        return Min_Time, Max_Time\n",
    "\n",
    "    # create dataframe from Min_time to Max_time\n",
    "    def create_final_df(self, datasets, Min_Time, Max_Time):\n",
    "        fitbit = pd.date_range(start=Min_Time, end=Max_Time, freq=self.time_interval_str, name='timestamp')\n",
    "        fitbit = pd.DataFrame(fitbit)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            fitbit = pd.merge(fitbit, dataset, how='outer', on='timestamp')\n",
    "            \n",
    "        fitbit['BM'] = fitbit['BM'].fillna(0)\n",
    "        fitbit['sleep'] = fitbit['sleep'].fillna(method='ffill')\n",
    "\n",
    "        return fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac24927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger(EEGProcessor, FitbitProcessor):\n",
    "   \n",
    "    def __init__(self, eeg_filepath, fitbit_folderpath, time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180):\n",
    "        # Initialize by calling parent class constructor\n",
    "        EEGProcessor.__init__(self, eeg_filepath, time_interval, eeg_remove_time_in_group)\n",
    "        FitbitProcessor.__init__(self, fitbit_folderpath, time_interval, BM_sum_minutes)\n",
    "        \n",
    "        self.eeg_filepath = eeg_filepath\n",
    "\n",
    "    # Merge EEG and Fitbit data\n",
    "    def merge_data(self):\n",
    "        # processing EEG data\n",
    "        eeg_data = pd.read_csv(self.eeg_filepath)\n",
    "        result_dfs = []\n",
    "        \n",
    "        # For all experiments in the eeg data csv file\n",
    "        for exp_id in range(3, len(eeg_data)):\n",
    "            processed_data = self.process_eeg_data(exp_id)\n",
    "            if processed_data is not None:\n",
    "                result_dfs.append(processed_data)\n",
    "                \n",
    "        if result_dfs:\n",
    "            combined_eeg = pd.concat(result_dfs)\n",
    "            combined_eeg.index = pd.to_datetime(combined_eeg.index)\n",
    "        \n",
    "        # processing Fitbit data\n",
    "        fitbit_data = self.process_fitbit_data()\n",
    "        fitbit_data.index = pd.to_datetime(fitbit_data.index)\n",
    "\n",
    "        # merging two dataframes\n",
    "        if 'combined_eeg' in locals() and not fitbit_data.empty:\n",
    "            merged_df = combined_eeg.merge(fitbit_data, left_index=True, right_index=True, how='left')\n",
    "            return merged_df\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae3d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_jm.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_JM\")\n",
    "\n",
    "final_jm = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_yh.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_YH\")\n",
    "\n",
    "final_yh = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SJ\")\n",
    "\n",
    "\n",
    "final_sj = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sa.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SA\")\n",
    "\n",
    "\n",
    "final_sa = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_bs.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_BS\")\n",
    "\n",
    "\n",
    "final_bs = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_mj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_MJ\")\n",
    "\n",
    "\n",
    "final_mj = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c59dbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha_wave</th>\n",
       "      <th>beta_wave</th>\n",
       "      <th>theta_wave</th>\n",
       "      <th>delta_wave</th>\n",
       "      <th>gamma_wave</th>\n",
       "      <th>attention</th>\n",
       "      <th>hrv</th>\n",
       "      <th>hr</th>\n",
       "      <th>coherence</th>\n",
       "      <th>SP ratio</th>\n",
       "      <th>BM</th>\n",
       "      <th>sleep</th>\n",
       "      <th>eda</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-10-16 10:39:20</th>\n",
       "      <td>98.366769</td>\n",
       "      <td>101.628072</td>\n",
       "      <td>96.312422</td>\n",
       "      <td>91.826384</td>\n",
       "      <td>94.410009</td>\n",
       "      <td>36.937500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.357143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.055192</td>\n",
       "      <td>78.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>5.14</td>\n",
       "      <td>-1.821261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-16 10:39:40</th>\n",
       "      <td>93.265881</td>\n",
       "      <td>99.750884</td>\n",
       "      <td>92.076881</td>\n",
       "      <td>85.074334</td>\n",
       "      <td>91.760712</td>\n",
       "      <td>64.531250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.310345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.083343</td>\n",
       "      <td>78.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>5.09</td>\n",
       "      <td>-1.771261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-16 10:40:00</th>\n",
       "      <td>92.458164</td>\n",
       "      <td>99.227658</td>\n",
       "      <td>90.810442</td>\n",
       "      <td>83.178958</td>\n",
       "      <td>91.300818</td>\n",
       "      <td>72.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.092690</td>\n",
       "      <td>78.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>5.03</td>\n",
       "      <td>-1.721261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-16 10:40:20</th>\n",
       "      <td>93.191097</td>\n",
       "      <td>98.719353</td>\n",
       "      <td>92.557253</td>\n",
       "      <td>85.024866</td>\n",
       "      <td>90.182428</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.172414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.066576</td>\n",
       "      <td>78.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>4.89</td>\n",
       "      <td>-1.711261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-16 10:40:40</th>\n",
       "      <td>92.830909</td>\n",
       "      <td>96.476331</td>\n",
       "      <td>91.692616</td>\n",
       "      <td>83.868441</td>\n",
       "      <td>87.118172</td>\n",
       "      <td>51.156250</td>\n",
       "      <td>32.892857</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.052171</td>\n",
       "      <td>78.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>4.74</td>\n",
       "      <td>-1.701261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-04 13:57:20</th>\n",
       "      <td>96.462818</td>\n",
       "      <td>101.984994</td>\n",
       "      <td>96.968858</td>\n",
       "      <td>89.793724</td>\n",
       "      <td>92.941361</td>\n",
       "      <td>81.242424</td>\n",
       "      <td>37.896552</td>\n",
       "      <td>82.413793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.051729</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.91</td>\n",
       "      <td>-2.583520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-04 13:57:40</th>\n",
       "      <td>94.894634</td>\n",
       "      <td>100.428706</td>\n",
       "      <td>95.300203</td>\n",
       "      <td>87.970316</td>\n",
       "      <td>91.439456</td>\n",
       "      <td>75.062500</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>77.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.053814</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>-2.563520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-04 13:58:00</th>\n",
       "      <td>94.042113</td>\n",
       "      <td>98.627097</td>\n",
       "      <td>94.294506</td>\n",
       "      <td>87.400819</td>\n",
       "      <td>90.092603</td>\n",
       "      <td>67.531250</td>\n",
       "      <td>26.833333</td>\n",
       "      <td>77.966667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.045947</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.78</td>\n",
       "      <td>-2.543520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-04 13:58:20</th>\n",
       "      <td>94.600222</td>\n",
       "      <td>100.409416</td>\n",
       "      <td>95.208922</td>\n",
       "      <td>88.280319</td>\n",
       "      <td>91.589297</td>\n",
       "      <td>79.218750</td>\n",
       "      <td>23.966667</td>\n",
       "      <td>78.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.054622</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.77</td>\n",
       "      <td>-2.520187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-04 13:58:40</th>\n",
       "      <td>94.867388</td>\n",
       "      <td>96.996391</td>\n",
       "      <td>94.351352</td>\n",
       "      <td>87.200485</td>\n",
       "      <td>86.951500</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>22.933333</td>\n",
       "      <td>80.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.028034</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>-2.496854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2867 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     alpha_wave   beta_wave  theta_wave  delta_wave  \\\n",
       "2023-10-16 10:39:20   98.366769  101.628072   96.312422   91.826384   \n",
       "2023-10-16 10:39:40   93.265881   99.750884   92.076881   85.074334   \n",
       "2023-10-16 10:40:00   92.458164   99.227658   90.810442   83.178958   \n",
       "2023-10-16 10:40:20   93.191097   98.719353   92.557253   85.024866   \n",
       "2023-10-16 10:40:40   92.830909   96.476331   91.692616   83.868441   \n",
       "...                         ...         ...         ...         ...   \n",
       "2023-12-04 13:57:20   96.462818  101.984994   96.968858   89.793724   \n",
       "2023-12-04 13:57:40   94.894634  100.428706   95.300203   87.970316   \n",
       "2023-12-04 13:58:00   94.042113   98.627097   94.294506   87.400819   \n",
       "2023-12-04 13:58:20   94.600222  100.409416   95.208922   88.280319   \n",
       "2023-12-04 13:58:40   94.867388   96.996391   94.351352   87.200485   \n",
       "\n",
       "                     gamma_wave  attention        hrv         hr  coherence  \\\n",
       "2023-10-16 10:39:20   94.410009  36.937500   0.000000  25.357143        0.0   \n",
       "2023-10-16 10:39:40   91.760712  64.531250   0.000000  84.310345        0.0   \n",
       "2023-10-16 10:40:00   91.300818  72.636364   0.000000  83.750000        0.0   \n",
       "2023-10-16 10:40:20   90.182428  65.500000   0.000000  76.172414        0.0   \n",
       "2023-10-16 10:40:40   87.118172  51.156250  32.892857  78.750000        0.0   \n",
       "...                         ...        ...        ...        ...        ...   \n",
       "2023-12-04 13:57:20   92.941361  81.242424  37.896552  82.413793        0.0   \n",
       "2023-12-04 13:57:40   91.439456  75.062500  30.500000  77.600000        0.0   \n",
       "2023-12-04 13:58:00   90.092603  67.531250  26.833333  77.966667        0.0   \n",
       "2023-12-04 13:58:20   91.589297  79.218750  23.966667  78.466667        0.0   \n",
       "2023-12-04 13:58:40   86.951500  54.000000  22.933333  80.533333        0.0   \n",
       "\n",
       "                     SP ratio    BM  sleep   eda  temperature  \n",
       "2023-10-16 10:39:20  1.055192  78.0   95.0  5.14    -1.821261  \n",
       "2023-10-16 10:39:40  1.083343  78.0   95.0  5.09    -1.771261  \n",
       "2023-10-16 10:40:00  1.092690  78.0   95.0  5.03    -1.721261  \n",
       "2023-10-16 10:40:20  1.066576  78.0   95.0  4.89    -1.711261  \n",
       "2023-10-16 10:40:40  1.052171  78.0   95.0  4.74    -1.701261  \n",
       "...                       ...   ...    ...   ...          ...  \n",
       "2023-12-04 13:57:20  1.051729   9.0   25.0  6.91    -2.583520  \n",
       "2023-12-04 13:57:40  1.053814   9.0   25.0  6.85    -2.563520  \n",
       "2023-12-04 13:58:00  1.045947   9.0   25.0  6.78    -2.543520  \n",
       "2023-12-04 13:58:20  1.054622   9.0   25.0  6.77    -2.520187  \n",
       "2023-12-04 13:58:40  1.028034   9.0   25.0  6.75    -2.496854  \n",
       "\n",
       "[2867 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.concat([final_jm, final_sj, final_bs, final_yh, final_mj, final_sa])\n",
    "dataset = dataset.sort_index()\n",
    "dataset = dataset.dropna()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(r'C:\\Users\\ballj\\OneDrive\\바탕 화면\\2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ac406",
   "metadata": {},
   "source": [
    "# 3. Error value preprocessing (10sec group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476bd661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EEGProcessor:\n",
    "     \n",
    "    # time_interval : Unified seconds with Fitbit data (ex. 10secs) \n",
    "    # remove_time_in_group : the criteria of processing error values in each time_interval group (ex. 7secs)\n",
    "    \n",
    "    def __init__(self, file_path, time_interval, remove_time_in_group):\n",
    "        self.time_interval = time_interval\n",
    "        self.remove_time_in_group = remove_time_in_group\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        self.EEG_report = pd.read_csv(file_path)\n",
    "\n",
    "    # List to dataframe (ex. brain waves)\n",
    "    def parse_raw_data(self, dataframe, col_name):\n",
    "        col_str = dataframe.iloc[0][col_name]\n",
    "        col_str = col_str.strip('[]')\n",
    "        col_list = [float(val) for val in col_str.split(',')]  # cause list is divided by comma\n",
    "        col_data = pd.DataFrame({col_name: col_list})\n",
    "        return col_data\n",
    "\n",
    "    # Experiment time calculating function\n",
    "    def time_difference(self, dataframe, start_time_col, finish_time_col):\n",
    "        start_time = datetime.strptime(dataframe.iloc[0][start_time_col], '%Y-%m-%d %H:%M:%S')\n",
    "        finish_time = datetime.strptime(dataframe.iloc[0][finish_time_col], '%Y-%m-%d %0H:%M:%S')\n",
    "\n",
    "        # time difference between two datatime objects\n",
    "        time_difference = (finish_time - start_time).total_seconds()\n",
    "        return time_difference\n",
    "    \n",
    "    # Comparing the experimental initial recognition error period and delete the part to be deleted\n",
    "    def count_initial_same_values(self, series):\n",
    "        initial_value = series.iloc[0]\n",
    "        count = 0\n",
    "        for value in series:\n",
    "            if value == initial_value:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        return count\n",
    "    \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_start_time_trash_sec(self, start_time):\n",
    "        # Plus 1 min and delete second in input time\n",
    "        rounded_time = start_time + timedelta(minutes=1) - timedelta(seconds=start_time.second)\n",
    "        time_difference = (rounded_time - start_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        # use the seconds over remove_time_in_group seconds\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "        \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_finish_time_trash_sec(self, finish_time):\n",
    "        # delete second in input time\n",
    "        rounded_time = finish_time - timedelta(seconds=finish_time.second)\n",
    "        time_difference = (finish_time - rounded_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "\n",
    "    # Rounding time to nearest time which can divided by time interval\n",
    "    def nearest_time_rounding(self, time):\n",
    "        seconds = time.second\n",
    "        # For example, find nearest value in 0, 10, 20, 30, 40, 50sec\n",
    "        time_points = [time_point for time_point in range(0,60, self.time_interval)]\n",
    "        nearest = min(time_points, key=lambda time_point: abs(time_point - seconds))\n",
    "        \n",
    "        if nearest == time_points[-1] and seconds >= (time_points[-1] + self.remove_time_in_group):\n",
    "            rounded_time = time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n",
    "        else:\n",
    "            rounded_time = time.replace(second=nearest, microsecond=0)\n",
    "\n",
    "        return rounded_time\n",
    "\n",
    "    # Make the same end time\n",
    "    def align_end_time(self, dataframe_1, dataframe_2):\n",
    "        if dataframe_1.index[-1] > dataframe_2.index[-1]:\n",
    "            dataframe_1 = dataframe_1[dataframe_1.index <= dataframe_2.index[-1]]\n",
    "\n",
    "        elif dataframe_1.index[-1] < dataframe_2.index[-1]:\n",
    "            dataframe_2 = dataframe_2[dataframe_2.index <= dataframe_1.index[-1]]\n",
    "\n",
    "        else: \n",
    "            pass # if two dataframe's endtime is same\n",
    "\n",
    "        return dataframe_1, dataframe_2\n",
    "\n",
    "    # Adjust start time and end time processing\n",
    "    # start time processing -> process_type : 0 , finish time processing -> process_type : -1\n",
    "    # start time processing -> process_start_time_trash_sec func , finish time processing -> process_finish_time_trash_sec func    \n",
    "    def adjust_time_index(self, process_type, dataframe, func):\n",
    "        remainder = func(dataframe.index[process_type])\n",
    "        \n",
    "        # the last data only shows one original data, so processing this problem\n",
    "        one_sec = timedelta(seconds=1)\n",
    "\n",
    "        if remainder == False:\n",
    "            # change time to nearest (Start time processing)\n",
    "            if process_type == 0 :\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type])\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "            \n",
    "            # change time to nearest (Finish time processing)\n",
    "            else:\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type]) - one_sec\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "        \n",
    "        # if remainder is under remove_time_in_group, just remove\n",
    "        else:\n",
    "            cutting_time = timedelta(seconds=remainder)\n",
    "            # Start time processing\n",
    "            if process_type == 0:\n",
    "                dataframe = dataframe[dataframe.index >= dataframe.index[process_type] + cutting_time]\n",
    "            \n",
    "            # Finish time processing\n",
    "            # make the seconds like 9, 19, 29...\n",
    "            else:\n",
    "                dataframe = dataframe[dataframe.index <= dataframe.index[-1] - cutting_time - one_sec]\n",
    "\n",
    "        return dataframe\n",
    "    \n",
    "    # Removing error values in group (brain waves and attention score)\n",
    "    def check_invalid_values(self, group):\n",
    "        # find error data length in brain wave\n",
    "        alpha_invalid_series = group['α_wave_raw_data'].diff().eq(0)\n",
    "        alpha_invalid_timestamps = group.index[alpha_invalid_series].tolist()\n",
    "\n",
    "        # find error data length in attention_raw_data\n",
    "        attention_invalid_series = group['attention_raw_data'] == 0\n",
    "        attention_invalid_timestamps = group.index[attention_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        alpha_invalid = has_long_invalid_duration(alpha_invalid_timestamps)\n",
    "        attention_invalid = has_long_invalid_duration(attention_invalid_timestamps)\n",
    "        \n",
    "        # make error values to missing values\n",
    "        if alpha_invalid or attention_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            valid_conditions = (\n",
    "                (group['α_wave_raw_data'].diff() != 0) & \n",
    "                (group['β_wave_raw_data'].diff() != 0) & \n",
    "                (group['θ_wave_raw_data'].diff() != 0) & \n",
    "                (group['δ_wave_raw_data'].diff() != 0) & \n",
    "                (group['γ_wave_raw_data'].diff() != 0) & \n",
    "                (group['attention_raw_data'] != 0)\n",
    "            )\n",
    "            return group[valid_conditions].mean()\n",
    "\n",
    "    # Removing error values in group (hr)\n",
    "    def check_invalid_values_other(self, group):\n",
    "        # find error data length in hr\n",
    "        hr_invalid_series = group['hr_raw_data'] == 0\n",
    "        hr_invalid_timestamps = group.index[hr_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        hr_invalid = has_long_invalid_duration(hr_invalid_timestamps)\n",
    "\n",
    "        # make error values to missing values\n",
    "        if hr_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            group = group[(group['hr_raw_data'] != 0)]\n",
    "            return group.mean()\n",
    "    \n",
    "    # Process EEG data\n",
    "    def process_eeg_data(self, experiment_id):\n",
    "        if experiment_id not in self.EEG_report.index:\n",
    "            return None\n",
    "\n",
    "        # all experiments in one df\n",
    "        EEG_report_sample = self.EEG_report.loc[[experiment_id],:]\n",
    "\n",
    "        # one dataframe for one column\n",
    "        cols = ['α_wave_raw_data', 'β_wave_raw_data', 'θ_wave_raw_data', 'δ_wave_raw_data', 'γ_wave_raw_data', 'attention_raw_data', 'hrv_raw_data', 'hr_raw_data', 'coherence_flag_raw_data']\n",
    "        parsed_dfs = [self.parse_raw_data(EEG_report_sample, col) for col in cols]\n",
    "\n",
    "        # calculate two interval second because there's two type of time interval in EEG data\n",
    "        interval_sec = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[0])\n",
    "        interval_sec_other = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[6])\n",
    "\n",
    "        # make two merged dataframe\n",
    "        merged_df = parsed_dfs[0].join(parsed_dfs[1:6])\n",
    "        merged_df_other = parsed_dfs[6].join(parsed_dfs[7:])\n",
    "\n",
    "        # experiment start time\n",
    "        start_time = datetime.strptime(EEG_report_sample.iloc[0]['meditation_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # change index to time index based on interval second\n",
    "        interval_sec, interval_sec_other = timedelta(seconds=round(interval_sec,2)), timedelta(seconds=round(interval_sec_other,2))\n",
    "        merged_df['time'] = [start_time + time * interval_sec for time in range(len(merged_df))]\n",
    "        merged_df_other['time'] = [start_time + time * interval_sec_other for time in range(len(merged_df_other))]\n",
    "        merged_df, merged_df_other = merged_df.set_index('time'), merged_df_other.set_index('time')\n",
    "        \n",
    "        # comparing the inital experiment error time\n",
    "        counts = [self.count_initial_same_values(merged_df[col]) for col in cols[:6]] + [self.count_initial_same_values(merged_df_other['hr_raw_data'])]\n",
    "        initial_error_times = [counts[error] * interval_sec.total_seconds() if error != 6 else counts[error] * interval_sec_other.total_seconds() for error in range(7)]\n",
    "        initial_error_time = timedelta(seconds=max(initial_error_times))\n",
    "\n",
    "        # dataset start time\n",
    "        real_start_time = start_time + initial_error_time\n",
    "        merged_df, merged_df_other = merged_df[merged_df.index > real_start_time], merged_df_other[merged_df_other.index > real_start_time]\n",
    "        merged_df.index, merged_df_other.index = merged_df.index.round('S'), merged_df_other.index.round('S')\n",
    "\n",
    "        # make the experiment end time same\n",
    "        merged_df, merged_df_other = self.align_end_time(merged_df, merged_df_other)\n",
    "\n",
    "        # start time process -> i : 0 , finish time process -> i : -1\n",
    "        # start time process -> process_start_time_trash_sec func , finish time process -> process_finish_time_trash_sec func\n",
    "        merged_df = self.adjust_time_index(0, merged_df, self.process_start_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(0, merged_df_other, self.process_start_time_trash_sec)\n",
    "        merged_df = self.adjust_time_index(-1, merged_df, self.process_finish_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(-1, merged_df_other, self.process_finish_time_trash_sec)\n",
    "\n",
    "        # grouping\n",
    "        grouped = merged_df.groupby(merged_df.index.floor(self.time_interval_str))\n",
    "        grouped_other = merged_df_other.groupby(merged_df_other.index.floor(self.time_interval_str))\n",
    "\n",
    "        result = grouped.apply(self.check_invalid_values)\n",
    "        result_other = grouped_other.apply(self.check_invalid_values_other)\n",
    "\n",
    "        # final EEG dataset including β/θ SP ratio\n",
    "        EEG_data_per_time_interval = result.merge(result_other, left_index=True, right_index=True)\n",
    "        EEG_data_per_time_interval['β/θ SP'] = EEG_data_per_time_interval['β_wave_raw_data'] / EEG_data_per_time_interval['θ_wave_raw_data']\n",
    "        \n",
    "        EEG_data_per_time_interval = EEG_data_per_time_interval.rename(columns={\n",
    "            'α_wave_raw_data':'alpha_wave',\n",
    "            'β_wave_raw_data':'beta_wave',\n",
    "            'θ_wave_raw_data':'theta_wave',\n",
    "            'δ_wave_raw_data':'delta_wave',\n",
    "            'γ_wave_raw_data':'gamma_wave',\n",
    "            'attention_raw_data' : 'attention',\n",
    "            'hrv_raw_data' : 'hrv',\n",
    "            'hr_raw_data' : 'hr',\n",
    "            'coherence_flag_raw_data' : 'coherence',\n",
    "            'β/θ SP' : 'SP ratio'\n",
    "        })\n",
    "\n",
    "        return EEG_data_per_time_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "class FitbitProcessor:\n",
    "    '''\n",
    "    time_interval : Unified seconds with EEG dataset (ex.10secs) \n",
    "    BM_sum_minutes\n",
    "    Body Movement feature was meaningless because the experiment was conducted usually while sitting on the chair.\n",
    "    So, created a new body movement feature as accumulated Body Movement value from previous time.\n",
    "    And, the previous time is the BM_sum_minutes variable. (ex. 3 hours)\n",
    "    '''\n",
    "    def __init__(self, folder_path, time_interval, BM_sum_minutes):\n",
    "        self.folder_path = folder_path\n",
    "        self.BM_sum_minutes = BM_sum_minutes\n",
    "        self.BM_sum_minutes_str = f'{BM_sum_minutes}T'\n",
    "        self.time_interval = time_interval\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        folder_patterns = [\n",
    "            \"Active Zone Minutes (AZM)/*\",\n",
    "            \"Sleep Score/*\",\n",
    "            \"Stress Journal/CEDA*\",\n",
    "            \"Temperature/Wrist Temperature - *\"\n",
    "        ]\n",
    "        self.things_path = [glob.glob(f\"{folder_path}/{pattern}\") for pattern in folder_patterns]\n",
    "        self.things_path = [item for sublist in self.things_path for item in sublist]\n",
    "        self.wt_count = len(glob.glob(f\"{folder_path}/Temperature/Wrist Temperature - *\"))\n",
    "        self.azm_count = len(glob.glob(f\"{folder_path}/Active Zone Minutes (AZM)/*\"))\n",
    "        self.sleep_count = len(glob.glob(f\"{folder_path}/Sleep Score/*\"))\n",
    "        self.eda_count = len(glob.glob(f\"{folder_path}/Stress Journal/CEDA*\"))\n",
    "        # original fitbit dataset's time interval is 1 min\n",
    "        self.original_interval = 60\n",
    "        self.num_timestamps = self.original_interval // self.time_interval\n",
    "        self.half_point = self.num_timestamps // 2\n",
    "    \n",
    "    def read_filtered_csv(self, path, columns):\n",
    "        name = pd.read_csv(path)\n",
    "        name = name[columns]\n",
    "        name[columns[0]] = pd.to_datetime(name[columns[0]])\n",
    "        return name\n",
    "    \n",
    "    def round_seconds(self, obj):\n",
    "        if obj.second % self.time_interval == 0:\n",
    "            return obj\n",
    "        else:\n",
    "            return obj - timedelta(seconds=obj.second % self.time_interval)\n",
    "    \n",
    "    def round_zero(self, datetime_obj):\n",
    "        datetime_obj = datetime_obj.replace(second=0)\n",
    "        return datetime_obj\n",
    "    \n",
    "    # Process whole fitbit data\n",
    "    def process_fitbit_data(self):\n",
    "        AZM_col = ['date_time', 'total_minutes']\n",
    "        sleep_col = ['timestamp', 'deep_sleep_in_minutes']\n",
    "        stress_col = ['timestamp', 'eda_level_real']\n",
    "        temp_col = ['recorded_time', 'temperature']\n",
    "        \n",
    "        # merge all features\n",
    "        things_col = [AZM_col] * self.azm_count + [sleep_col] * self.sleep_count + [stress_col] * self.eda_count + [temp_col] * self.wt_count\n",
    "        things = [self.read_filtered_csv(path, col) for path, col in zip(self.things_path, things_col)]\n",
    "\n",
    "        # if there's no wrist temperature\n",
    "        if self.wt_count == 0:\n",
    "            # if there's no eda data\n",
    "            # there was no eda in two subjects' fitbit data\n",
    "            if self.eda_count == 0:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep])\n",
    "                \n",
    "                # make final dataframe\n",
    "                df = self.create_final_df([azm, sleep], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                df = df.assign(eda=np.nan)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            else:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda])\n",
    "\n",
    "                df = self.create_final_df([azm, sleep, eda], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                return df\n",
    "            \n",
    "        # if there's no Active Zone Minutes data\n",
    "        elif self.azm_count == 0:\n",
    "            sleep = self.process_sleep(things[self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(BM=np.nan)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # if there's no sleep data\n",
    "        elif self.sleep_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(sleep=np.nan)\n",
    "\n",
    "            return df            \n",
    "        \n",
    "        # if there's no eda data\n",
    "        elif self.eda_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(eda=np.nan)\n",
    "\n",
    "            return df\n",
    "            \n",
    "        # if there's no error in data file\n",
    "        else:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "\n",
    "            return df\n",
    "    \n",
    "    # Process Active Zone Minutes data\n",
    "    def process_azm(self, azm):\n",
    "\n",
    "        azm = pd.concat(azm, axis=0)\n",
    "        azm = azm.rename(columns={'date_time':'timestamp', 'total_minutes':'BM'})\n",
    "        time_list = []\n",
    "        body_movement = []\n",
    "\n",
    "        for time in azm['timestamp']:\n",
    "            start_timestamp = time - timedelta(seconds=(self.half_point * self.time_interval))\n",
    "\n",
    "            for number in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (number*self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "        for bm in azm['BM']:\n",
    "            for _ in range(self.num_timestamps):\n",
    "                body_movement.append(bm)\n",
    "\n",
    "        azm_list = {'timestamp': time_list, 'BM': body_movement}\n",
    "        azm_final = pd.DataFrame(azm_list)\n",
    "\n",
    "#         azm_final['timestamp'] = pd.to_datetime(azm_final['timestamp'])\n",
    "#         azm_final.set_index('timestamp', inplace=True)\n",
    "#         azm_final = azm_final.resample(self.time_interval_str).asfreq().fillna(0)\n",
    "#         azm_final['new_BM'] = azm_final['BM'].rolling(self.BM_sum_minutes_str, closed='right').sum()\n",
    "#         azm_final = azm_final.drop(['BM'], axis=1)\n",
    "#         azm_final = azm_final.rename(columns={'new_BM':'BM'})\n",
    "#         azm_final = azm_final.astype({'BM':'int'})\n",
    "#         azm_final.reset_index(inplace=True)\n",
    "        \n",
    "        return azm_final\n",
    "    \n",
    "    # Process sleep data (Deep sleep in minutes)\n",
    "    def process_sleep(self, sleep):\n",
    "        sleep = pd.concat(sleep, axis=0)\n",
    "        sleep['timestamp'] = [\n",
    "                self.round_zero(datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S')) for time in sleep['timestamp']\n",
    "            ]\n",
    "            \n",
    "        sleep = sleep.rename(columns={'deep_sleep_in_minutes':'sleep'})\n",
    "        return sleep\n",
    "    \n",
    "    # Process eda data\n",
    "    def process_eda(self, eda):\n",
    "        eda = pd.concat(eda, axis=0)\n",
    "        eda['timestamp'] = [\n",
    "            self.round_seconds(\n",
    "                datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=4)\n",
    "            ) for time in eda['timestamp']]\n",
    "\n",
    "        # EDA Bilinear Interpolation\n",
    "        time_list = []\n",
    "        eda_list = []\n",
    "\n",
    "        for time in range(1, len(eda['timestamp']) - 1):\n",
    "            start_timestamp = eda.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "\n",
    "            for num in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "                if num < self.half_point:\n",
    "                    weight = (self.half_point - num) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] - ((eda.iloc[time, 1] - eda.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                elif num == self.half_point:\n",
    "                    value = eda.iloc[time,1]\n",
    "\n",
    "                else:\n",
    "                    weight = (num - self.half_point) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] + ((eda.iloc[time + 1, 1] - eda.iloc[time, 1]) * weight)\n",
    "\n",
    "                eda_list.append(round(value, 2))\n",
    "\n",
    "        eda_list = {'timestamp': time_list, 'eda': eda_list}\n",
    "        eda_final = pd.DataFrame(eda_list)\n",
    "\n",
    "        return eda_final\n",
    "    \n",
    "    # Process temperature data\n",
    "    def process_temperature(self, temperature):\n",
    "        #Temperature\n",
    "        temp = pd.concat(temperature, axis=0)\n",
    "        temp = temp.rename(columns={'recorded_time':'timestamp'})\n",
    "\n",
    "        # Temperature Bilinear Interpolation\n",
    "        time_list = []\n",
    "        temp_list = []\n",
    "\n",
    "        for time in range(1, len(temp['timestamp']) - 1):\n",
    "            if self.half_point % 2 != 0:\n",
    "                start_timestamp = temp.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "                for num in range(self.num_timestamps):\n",
    "                    new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                    time_list.append(new_timestamp)\n",
    "\n",
    "                    if num < self.half_point:\n",
    "                        weight = (self.half_point - num) / self.num_timestamps \n",
    "                        value = temp.iloc[time, 1] - ((temp.iloc[time, 1] - temp.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                    elif num == self.half_point:\n",
    "                        value = temp.iloc[time,1]\n",
    "\n",
    "                    else:\n",
    "                        weight = (num - self.half_point) / self.num_timestamps\n",
    "                        value = temp.iloc[time, 1] + ((temp.iloc[time + 1, 1] - temp.iloc[time, 1]) * weight)\n",
    "\n",
    "                    temp_list.append(round(value, 6))\n",
    "\n",
    "        temp_list = {'timestamp': time_list, 'temperature': temp_list}\n",
    "        temp_final = pd.DataFrame(temp_list)\n",
    "\n",
    "        return temp_final\n",
    "    \n",
    "    # find minimum and maximum time of whole feature\n",
    "    def find_time_bounds(self, dataframes):\n",
    "        min_times = []\n",
    "        max_times = []\n",
    "        \n",
    "        for df in dataframes:\n",
    "            if not df.empty:\n",
    "                min_times.append(df['timestamp'].min())\n",
    "                max_times.append(df['timestamp'].max())\n",
    "                \n",
    "        if not min_times or not max_times:\n",
    "            Min_Time = pd.Timestamp.now(tz='UTC')\n",
    "            Max_Time = pd.Timestamp.now(tz='UTC')\n",
    "        else:\n",
    "            Min_Time = min(min_times)\n",
    "            Max_Time = max(max_times)\n",
    "\n",
    "        return Min_Time, Max_Time\n",
    "\n",
    "    # create dataframe from Min_time to Max_time\n",
    "    def create_final_df(self, datasets, Min_Time, Max_Time):\n",
    "        fitbit = pd.date_range(start=Min_Time, end=Max_Time, freq=self.time_interval_str, name='timestamp')\n",
    "        fitbit = pd.DataFrame(fitbit)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            fitbit = pd.merge(fitbit, dataset, how='outer', on='timestamp')\n",
    "            \n",
    "        fitbit['BM'] = fitbit['BM'].fillna(0)\n",
    "        fitbit['sleep'] = fitbit['sleep'].fillna(method='ffill')\n",
    "\n",
    "        return fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger(EEGProcessor, FitbitProcessor):\n",
    "    '''\n",
    "    We've tested some hyperparameters, and \"time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180\" have shown the best R-squared score.\n",
    "    \n",
    "    [Tested hyperparameters]\n",
    "    1. Time interval : 10, 12, 15, 20sec & eeg_remove_time_in_group : 7, 9, 12, 16sec (7~80% proportion of time interval)\n",
    "    R-squared score was best when we split the dataset into 10 seconds group.\n",
    "    \n",
    "    2. BM (Body Movement) sum minutes : 1h, 1h 30m, 2h, 2h 30m, 3h\n",
    "    R-squared score was best when we set up the BM (Body Movement) sum minutes as 3 hours.     \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eeg_filepath, fitbit_folderpath, time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180):\n",
    "        # Initialize by calling parent class constructor\n",
    "        EEGProcessor.__init__(self, eeg_filepath, time_interval, eeg_remove_time_in_group)\n",
    "        FitbitProcessor.__init__(self, fitbit_folderpath, time_interval, BM_sum_minutes)\n",
    "        \n",
    "        self.eeg_filepath = eeg_filepath\n",
    "\n",
    "    # Merge EEG and Fitbit data\n",
    "    def merge_data(self):\n",
    "        # processing EEG data\n",
    "        eeg_data = pd.read_csv(self.eeg_filepath)\n",
    "        result_dfs = []\n",
    "        \n",
    "        # For all experiments in the eeg data csv file\n",
    "        for exp_id in range(3, len(eeg_data)):\n",
    "            processed_data = self.process_eeg_data(exp_id)\n",
    "            if processed_data is not None:\n",
    "                result_dfs.append(processed_data)\n",
    "                \n",
    "        if result_dfs:\n",
    "            combined_eeg = pd.concat(result_dfs)\n",
    "            combined_eeg.index = pd.to_datetime(combined_eeg.index)\n",
    "        \n",
    "        # processing Fitbit data\n",
    "        fitbit_data = self.process_fitbit_data()\n",
    "        fitbit_data.index = pd.to_datetime(fitbit_data.index)\n",
    "\n",
    "        # merging two dataframes\n",
    "        if 'combined_eeg' in locals() and not fitbit_data.empty:\n",
    "            merged_df = combined_eeg.merge(fitbit_data, left_index=True, right_index=True, how='left')\n",
    "            return merged_df\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f385926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_jm.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_JM\")\n",
    "\n",
    "final_jm = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_yh.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_YH\")\n",
    "\n",
    "final_yh = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SJ\")\n",
    "\n",
    "\n",
    "final_sj = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sa.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SA\")\n",
    "\n",
    "\n",
    "final_sa = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_bs.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_BS\")\n",
    "\n",
    "\n",
    "final_bs = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_mj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_MJ\")\n",
    "\n",
    "\n",
    "final_mj = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdccdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([final_jm, final_sj, final_bs, final_yh, final_mj, final_sa])\n",
    "dataset = dataset.sort_index()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(r'C:\\Users\\ballj\\OneDrive\\바탕 화면\\3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca9992b",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering (10sec group) - Same as Automation Code for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50160b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EEGProcessor:\n",
    "     \n",
    "    # time_interval : Unified seconds with Fitbit data (ex. 10secs) \n",
    "    # remove_time_in_group : the criteria of processing error values in each time_interval group (ex. 7secs)\n",
    "    \n",
    "    def __init__(self, file_path, time_interval, remove_time_in_group):\n",
    "        self.time_interval = time_interval\n",
    "        self.remove_time_in_group = remove_time_in_group\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        self.EEG_report = pd.read_csv(file_path)\n",
    "\n",
    "    # List to dataframe (ex. brain waves)\n",
    "    def parse_raw_data(self, dataframe, col_name):\n",
    "        col_str = dataframe.iloc[0][col_name]\n",
    "        col_str = col_str.strip('[]')\n",
    "        col_list = [float(val) for val in col_str.split(',')]  # cause list is divided by comma\n",
    "        col_data = pd.DataFrame({col_name: col_list})\n",
    "        return col_data\n",
    "\n",
    "    # Experiment time calculating function\n",
    "    def time_difference(self, dataframe, start_time_col, finish_time_col):\n",
    "        start_time = datetime.strptime(dataframe.iloc[0][start_time_col], '%Y-%m-%d %H:%M:%S')\n",
    "        finish_time = datetime.strptime(dataframe.iloc[0][finish_time_col], '%Y-%m-%d %0H:%M:%S')\n",
    "\n",
    "        # time difference between two datatime objects\n",
    "        time_difference = (finish_time - start_time).total_seconds()\n",
    "        return time_difference\n",
    "    \n",
    "    # Comparing the experimental initial recognition error period and delete the part to be deleted\n",
    "    def count_initial_same_values(self, series):\n",
    "        initial_value = series.iloc[0]\n",
    "        count = 0\n",
    "        for value in series:\n",
    "            if value == initial_value:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        return count\n",
    "    \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_start_time_trash_sec(self, start_time):\n",
    "        # Plus 1 min and delete second in input time\n",
    "        rounded_time = start_time + timedelta(minutes=1) - timedelta(seconds=start_time.second)\n",
    "        time_difference = (rounded_time - start_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        # use the seconds over remove_time_in_group seconds\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "        \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_finish_time_trash_sec(self, finish_time):\n",
    "        # delete second in input time\n",
    "        rounded_time = finish_time - timedelta(seconds=finish_time.second)\n",
    "        time_difference = (finish_time - rounded_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "\n",
    "    # Rounding time to nearest time which can divided by time interval\n",
    "    def nearest_time_rounding(self, time):\n",
    "        seconds = time.second\n",
    "        # For example, find nearest value in 0, 10, 20, 30, 40, 50sec\n",
    "        time_points = [time_point for time_point in range(0,60, self.time_interval)]\n",
    "        nearest = min(time_points, key=lambda time_point: abs(time_point - seconds))\n",
    "        \n",
    "        if nearest == time_points[-1] and seconds >= (time_points[-1] + self.remove_time_in_group):\n",
    "            rounded_time = time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n",
    "        else:\n",
    "            rounded_time = time.replace(second=nearest, microsecond=0)\n",
    "\n",
    "        return rounded_time\n",
    "\n",
    "    # Make the same end time\n",
    "    def align_end_time(self, dataframe_1, dataframe_2):\n",
    "        if dataframe_1.index[-1] > dataframe_2.index[-1]:\n",
    "            dataframe_1 = dataframe_1[dataframe_1.index <= dataframe_2.index[-1]]\n",
    "\n",
    "        elif dataframe_1.index[-1] < dataframe_2.index[-1]:\n",
    "            dataframe_2 = dataframe_2[dataframe_2.index <= dataframe_1.index[-1]]\n",
    "\n",
    "        else: \n",
    "            pass # if two dataframe's endtime is same\n",
    "\n",
    "        return dataframe_1, dataframe_2\n",
    "\n",
    "    # Adjust start time and end time processing\n",
    "    # start time processing -> process_type : 0 , finish time processing -> process_type : -1\n",
    "    # start time processing -> process_start_time_trash_sec func , finish time processing -> process_finish_time_trash_sec func    \n",
    "    def adjust_time_index(self, process_type, dataframe, func):\n",
    "        remainder = func(dataframe.index[process_type])\n",
    "        \n",
    "        # the last data only shows one original data, so processing this problem\n",
    "        one_sec = timedelta(seconds=1)\n",
    "\n",
    "        if remainder == False:\n",
    "            # change time to nearest (Start time processing)\n",
    "            if process_type == 0 :\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type])\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "            \n",
    "            # change time to nearest (Finish time processing)\n",
    "            else:\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type]) - one_sec\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "        \n",
    "        # if remainder is under remove_time_in_group, just remove\n",
    "        else:\n",
    "            cutting_time = timedelta(seconds=remainder)\n",
    "            # Start time processing\n",
    "            if process_type == 0:\n",
    "                dataframe = dataframe[dataframe.index >= dataframe.index[process_type] + cutting_time]\n",
    "            \n",
    "            # Finish time processing\n",
    "            # make the seconds like 9, 19, 29...\n",
    "            else:\n",
    "                dataframe = dataframe[dataframe.index <= dataframe.index[-1] - cutting_time - one_sec]\n",
    "\n",
    "        return dataframe\n",
    "    \n",
    "    # Removing error values in group (brain waves and attention score)\n",
    "    def check_invalid_values(self, group):\n",
    "        # find error data length in brain wave\n",
    "        alpha_invalid_series = group['α_wave_raw_data'].diff().eq(0)\n",
    "        alpha_invalid_timestamps = group.index[alpha_invalid_series].tolist()\n",
    "\n",
    "        # find error data length in attention_raw_data\n",
    "        attention_invalid_series = group['attention_raw_data'] == 0\n",
    "        attention_invalid_timestamps = group.index[attention_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        alpha_invalid = has_long_invalid_duration(alpha_invalid_timestamps)\n",
    "        attention_invalid = has_long_invalid_duration(attention_invalid_timestamps)\n",
    "        \n",
    "        # make error values to missing values\n",
    "        if alpha_invalid or attention_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            valid_conditions = (\n",
    "                (group['α_wave_raw_data'].diff() != 0) & \n",
    "                (group['β_wave_raw_data'].diff() != 0) & \n",
    "                (group['θ_wave_raw_data'].diff() != 0) & \n",
    "                (group['δ_wave_raw_data'].diff() != 0) & \n",
    "                (group['γ_wave_raw_data'].diff() != 0) & \n",
    "                (group['attention_raw_data'] != 0)\n",
    "            )\n",
    "            return group[valid_conditions].mean()\n",
    "\n",
    "    # Removing error values in group (hr)\n",
    "    def check_invalid_values_other(self, group):\n",
    "        # find error data length in hr\n",
    "        hr_invalid_series = group['hr_raw_data'] == 0\n",
    "        hr_invalid_timestamps = group.index[hr_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        hr_invalid = has_long_invalid_duration(hr_invalid_timestamps)\n",
    "\n",
    "        # make error values to missing values\n",
    "        if hr_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            group = group[(group['hr_raw_data'] != 0)]\n",
    "            return group.mean()\n",
    "    \n",
    "    # Process EEG data\n",
    "    def process_eeg_data(self, experiment_id):\n",
    "        if experiment_id not in self.EEG_report.index:\n",
    "            return None\n",
    "\n",
    "        # all experiments in one df\n",
    "        EEG_report_sample = self.EEG_report.loc[[experiment_id],:]\n",
    "\n",
    "        # one dataframe for one column\n",
    "        cols = ['α_wave_raw_data', 'β_wave_raw_data', 'θ_wave_raw_data', 'δ_wave_raw_data', 'γ_wave_raw_data', 'attention_raw_data', 'hrv_raw_data', 'hr_raw_data', 'coherence_flag_raw_data']\n",
    "        parsed_dfs = [self.parse_raw_data(EEG_report_sample, col) for col in cols]\n",
    "\n",
    "        # calculate two interval second because there's two type of time interval in EEG data\n",
    "        interval_sec = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[0])\n",
    "        interval_sec_other = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[6])\n",
    "\n",
    "        # make two merged dataframe\n",
    "        merged_df = parsed_dfs[0].join(parsed_dfs[1:6])\n",
    "        merged_df_other = parsed_dfs[6].join(parsed_dfs[7:])\n",
    "\n",
    "        # experiment start time\n",
    "        start_time = datetime.strptime(EEG_report_sample.iloc[0]['meditation_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # change index to time index based on interval second\n",
    "        interval_sec, interval_sec_other = timedelta(seconds=round(interval_sec,2)), timedelta(seconds=round(interval_sec_other,2))\n",
    "        merged_df['time'] = [start_time + time * interval_sec for time in range(len(merged_df))]\n",
    "        merged_df_other['time'] = [start_time + time * interval_sec_other for time in range(len(merged_df_other))]\n",
    "        merged_df, merged_df_other = merged_df.set_index('time'), merged_df_other.set_index('time')\n",
    "        \n",
    "        # comparing the inital experiment error time\n",
    "        counts = [self.count_initial_same_values(merged_df[col]) for col in cols[:6]] + [self.count_initial_same_values(merged_df_other['hr_raw_data'])]\n",
    "        initial_error_times = [counts[error] * interval_sec.total_seconds() if error != 6 else counts[error] * interval_sec_other.total_seconds() for error in range(7)]\n",
    "        initial_error_time = timedelta(seconds=max(initial_error_times))\n",
    "\n",
    "        # dataset start time\n",
    "        real_start_time = start_time + initial_error_time\n",
    "        merged_df, merged_df_other = merged_df[merged_df.index > real_start_time], merged_df_other[merged_df_other.index > real_start_time]\n",
    "        merged_df.index, merged_df_other.index = merged_df.index.round('S'), merged_df_other.index.round('S')\n",
    "\n",
    "        # make the experiment end time same\n",
    "        merged_df, merged_df_other = self.align_end_time(merged_df, merged_df_other)\n",
    "\n",
    "        # start time process -> i : 0 , finish time process -> i : -1\n",
    "        # start time process -> process_start_time_trash_sec func , finish time process -> process_finish_time_trash_sec func\n",
    "        merged_df = self.adjust_time_index(0, merged_df, self.process_start_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(0, merged_df_other, self.process_start_time_trash_sec)\n",
    "        merged_df = self.adjust_time_index(-1, merged_df, self.process_finish_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(-1, merged_df_other, self.process_finish_time_trash_sec)\n",
    "\n",
    "        # grouping\n",
    "        grouped = merged_df.groupby(merged_df.index.floor(self.time_interval_str))\n",
    "        grouped_other = merged_df_other.groupby(merged_df_other.index.floor(self.time_interval_str))\n",
    "\n",
    "        result = grouped.apply(self.check_invalid_values)\n",
    "        result_other = grouped_other.apply(self.check_invalid_values_other)\n",
    "\n",
    "        # final EEG dataset including β/θ SP ratio\n",
    "        EEG_data_per_time_interval = result.merge(result_other, left_index=True, right_index=True)\n",
    "        EEG_data_per_time_interval['β/θ SP'] = EEG_data_per_time_interval['β_wave_raw_data'] / EEG_data_per_time_interval['θ_wave_raw_data']\n",
    "        \n",
    "        EEG_data_per_time_interval = EEG_data_per_time_interval.rename(columns={\n",
    "            'α_wave_raw_data':'alpha_wave',\n",
    "            'β_wave_raw_data':'beta_wave',\n",
    "            'θ_wave_raw_data':'theta_wave',\n",
    "            'δ_wave_raw_data':'delta_wave',\n",
    "            'γ_wave_raw_data':'gamma_wave',\n",
    "            'attention_raw_data' : 'attention',\n",
    "            'hrv_raw_data' : 'hrv',\n",
    "            'hr_raw_data' : 'hr',\n",
    "            'coherence_flag_raw_data' : 'coherence',\n",
    "            'β/θ SP' : 'SP ratio'\n",
    "        })\n",
    "\n",
    "        return EEG_data_per_time_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "class FitbitProcessor:\n",
    "    '''\n",
    "    time_interval : Unified seconds with EEG dataset (ex.10secs) \n",
    "    BM_sum_minutes\n",
    "    Body Movement feature was meaningless because the experiment was conducted usually while sitting on the chair.\n",
    "    So, created a new body movement feature as accumulated Body Movement value from previous time.\n",
    "    And, the previous time is the BM_sum_minutes variable. (ex. 3 hours)\n",
    "    '''\n",
    "    def __init__(self, folder_path, time_interval, BM_sum_minutes):\n",
    "        self.folder_path = folder_path\n",
    "        self.BM_sum_minutes = BM_sum_minutes\n",
    "        self.BM_sum_minutes_str = f'{BM_sum_minutes}T'\n",
    "        self.time_interval = time_interval\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        folder_patterns = [\n",
    "            \"Active Zone Minutes (AZM)/*\",\n",
    "            \"Sleep Score/*\",\n",
    "            \"Stress Journal/CEDA*\",\n",
    "            \"Temperature/Wrist Temperature - *\"\n",
    "        ]\n",
    "        self.things_path = [glob.glob(f\"{folder_path}/{pattern}\") for pattern in folder_patterns]\n",
    "        self.things_path = [item for sublist in self.things_path for item in sublist]\n",
    "        self.wt_count = len(glob.glob(f\"{folder_path}/Temperature/Wrist Temperature - *\"))\n",
    "        self.azm_count = len(glob.glob(f\"{folder_path}/Active Zone Minutes (AZM)/*\"))\n",
    "        self.sleep_count = len(glob.glob(f\"{folder_path}/Sleep Score/*\"))\n",
    "        self.eda_count = len(glob.glob(f\"{folder_path}/Stress Journal/CEDA*\"))\n",
    "        # original fitbit dataset's time interval is 1 min\n",
    "        self.original_interval = 60\n",
    "        self.num_timestamps = self.original_interval // self.time_interval\n",
    "        self.half_point = self.num_timestamps // 2\n",
    "    \n",
    "    def read_filtered_csv(self, path, columns):\n",
    "        name = pd.read_csv(path)\n",
    "        name = name[columns]\n",
    "        name[columns[0]] = pd.to_datetime(name[columns[0]])\n",
    "        return name\n",
    "    \n",
    "    def round_seconds(self, obj):\n",
    "        if obj.second % self.time_interval == 0:\n",
    "            return obj\n",
    "        else:\n",
    "            return obj - timedelta(seconds=obj.second % self.time_interval)\n",
    "    \n",
    "    def round_zero(self, datetime_obj):\n",
    "        datetime_obj = datetime_obj.replace(second=0)\n",
    "        return datetime_obj\n",
    "    \n",
    "    # Process whole fitbit data\n",
    "    def process_fitbit_data(self):\n",
    "        AZM_col = ['date_time', 'total_minutes']\n",
    "        sleep_col = ['timestamp', 'deep_sleep_in_minutes']\n",
    "        stress_col = ['timestamp', 'eda_level_real']\n",
    "        temp_col = ['recorded_time', 'temperature']\n",
    "        \n",
    "        # merge all features\n",
    "        things_col = [AZM_col] * self.azm_count + [sleep_col] * self.sleep_count + [stress_col] * self.eda_count + [temp_col] * self.wt_count\n",
    "        things = [self.read_filtered_csv(path, col) for path, col in zip(self.things_path, things_col)]\n",
    "\n",
    "        # if there's no wrist temperature\n",
    "        if self.wt_count == 0:\n",
    "            # if there's no eda data\n",
    "            # there was no eda in two subjects' fitbit data\n",
    "            if self.eda_count == 0:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep])\n",
    "                \n",
    "                # make final dataframe\n",
    "                df = self.create_final_df([azm, sleep], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                df = df.assign(eda=np.nan)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            else:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda])\n",
    "\n",
    "                df = self.create_final_df([azm, sleep, eda], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                return df\n",
    "            \n",
    "        # if there's no Active Zone Minutes data\n",
    "        elif self.azm_count == 0:\n",
    "            sleep = self.process_sleep(things[self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(BM=np.nan)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # if there's no sleep data\n",
    "        elif self.sleep_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(sleep=np.nan)\n",
    "\n",
    "            return df            \n",
    "        \n",
    "        # if there's no eda data\n",
    "        elif self.eda_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(eda=np.nan)\n",
    "\n",
    "            return df\n",
    "            \n",
    "        # if there's no error in data file\n",
    "        else:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "\n",
    "            return df\n",
    "    \n",
    "    # Process Active Zone Minutes data\n",
    "    def process_azm(self, azm):\n",
    "\n",
    "        azm = pd.concat(azm, axis=0)\n",
    "        azm = azm.rename(columns={'date_time':'timestamp', 'total_minutes':'BM'})\n",
    "        time_list = []\n",
    "        body_movement = []\n",
    "\n",
    "        for time in azm['timestamp']:\n",
    "            start_timestamp = time - timedelta(seconds=(self.half_point * self.time_interval))\n",
    "\n",
    "            for number in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (number*self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "        for bm in azm['BM']:\n",
    "            for _ in range(self.num_timestamps):\n",
    "                body_movement.append(bm)\n",
    "\n",
    "        azm_list = {'timestamp': time_list, 'BM': body_movement}\n",
    "        azm_final = pd.DataFrame(azm_list)\n",
    "\n",
    "        azm_final['timestamp'] = pd.to_datetime(azm_final['timestamp'])\n",
    "        azm_final.set_index('timestamp', inplace=True)\n",
    "        azm_final = azm_final.resample(self.time_interval_str).asfreq().fillna(0)\n",
    "        azm_final['new_BM'] = azm_final['BM'].rolling(self.BM_sum_minutes_str, closed='right').sum()\n",
    "        azm_final = azm_final.drop(['BM'], axis=1)\n",
    "        azm_final = azm_final.rename(columns={'new_BM':'BM'})\n",
    "        azm_final = azm_final.astype({'BM':'int'})\n",
    "        azm_final.reset_index(inplace=True)\n",
    "        \n",
    "        return azm_final\n",
    "    \n",
    "    # Process sleep data (Deep sleep in minutes)\n",
    "    def process_sleep(self, sleep):\n",
    "        sleep = pd.concat(sleep, axis=0)\n",
    "        sleep['timestamp'] = [\n",
    "                self.round_zero(datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S')) for time in sleep['timestamp']\n",
    "            ]\n",
    "            \n",
    "        sleep = sleep.rename(columns={'deep_sleep_in_minutes':'sleep'})\n",
    "        return sleep\n",
    "    \n",
    "    # Process eda data\n",
    "    def process_eda(self, eda):\n",
    "        eda = pd.concat(eda, axis=0)\n",
    "        eda['timestamp'] = [\n",
    "            self.round_seconds(\n",
    "                datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=4)\n",
    "            ) for time in eda['timestamp']]\n",
    "\n",
    "        # EDA Bilinear Interpolation\n",
    "        time_list = []\n",
    "        eda_list = []\n",
    "\n",
    "        for time in range(1, len(eda['timestamp']) - 1):\n",
    "            start_timestamp = eda.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "\n",
    "            for num in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "                if num < self.half_point:\n",
    "                    weight = (self.half_point - num) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] - ((eda.iloc[time, 1] - eda.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                elif num == self.half_point:\n",
    "                    value = eda.iloc[time,1]\n",
    "\n",
    "                else:\n",
    "                    weight = (num - self.half_point) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] + ((eda.iloc[time + 1, 1] - eda.iloc[time, 1]) * weight)\n",
    "\n",
    "                eda_list.append(round(value, 2))\n",
    "\n",
    "        eda_list = {'timestamp': time_list, 'eda': eda_list}\n",
    "        eda_final = pd.DataFrame(eda_list)\n",
    "\n",
    "        return eda_final\n",
    "    \n",
    "    # Process temperature data\n",
    "    def process_temperature(self, temperature):\n",
    "        #Temperature\n",
    "        temp = pd.concat(temperature, axis=0)\n",
    "        temp = temp.rename(columns={'recorded_time':'timestamp'})\n",
    "\n",
    "        # Temperature Bilinear Interpolation\n",
    "        time_list = []\n",
    "        temp_list = []\n",
    "\n",
    "        for time in range(1, len(temp['timestamp']) - 1):\n",
    "            if self.half_point % 2 != 0:\n",
    "                start_timestamp = temp.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "                for num in range(self.num_timestamps):\n",
    "                    new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                    time_list.append(new_timestamp)\n",
    "\n",
    "                    if num < self.half_point:\n",
    "                        weight = (self.half_point - num) / self.num_timestamps \n",
    "                        value = temp.iloc[time, 1] - ((temp.iloc[time, 1] - temp.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                    elif num == self.half_point:\n",
    "                        value = temp.iloc[time,1]\n",
    "\n",
    "                    else:\n",
    "                        weight = (num - self.half_point) / self.num_timestamps\n",
    "                        value = temp.iloc[time, 1] + ((temp.iloc[time + 1, 1] - temp.iloc[time, 1]) * weight)\n",
    "\n",
    "                    temp_list.append(round(value, 6))\n",
    "\n",
    "        temp_list = {'timestamp': time_list, 'temperature': temp_list}\n",
    "        temp_final = pd.DataFrame(temp_list)\n",
    "\n",
    "        return temp_final\n",
    "    \n",
    "    # find minimum and maximum time of whole feature\n",
    "    def find_time_bounds(self, dataframes):\n",
    "        min_times = []\n",
    "        max_times = []\n",
    "        \n",
    "        for df in dataframes:\n",
    "            if not df.empty:\n",
    "                min_times.append(df['timestamp'].min())\n",
    "                max_times.append(df['timestamp'].max())\n",
    "                \n",
    "        if not min_times or not max_times:\n",
    "            Min_Time = pd.Timestamp.now(tz='UTC')\n",
    "            Max_Time = pd.Timestamp.now(tz='UTC')\n",
    "        else:\n",
    "            Min_Time = min(min_times)\n",
    "            Max_Time = max(max_times)\n",
    "\n",
    "        return Min_Time, Max_Time\n",
    "\n",
    "    # create dataframe from Min_time to Max_time\n",
    "    def create_final_df(self, datasets, Min_Time, Max_Time):\n",
    "        fitbit = pd.date_range(start=Min_Time, end=Max_Time, freq=self.time_interval_str, name='timestamp')\n",
    "        fitbit = pd.DataFrame(fitbit)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            fitbit = pd.merge(fitbit, dataset, how='outer', on='timestamp')\n",
    "            \n",
    "        fitbit['BM'] = fitbit['BM'].fillna(0)\n",
    "        fitbit['sleep'] = fitbit['sleep'].fillna(method='ffill')\n",
    "\n",
    "        return fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf869ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger(EEGProcessor, FitbitProcessor):\n",
    "    '''\n",
    "    We've tested some hyperparameters, and \"time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180\" have shown the best R-squared score.\n",
    "    \n",
    "    [Tested hyperparameters]\n",
    "    1. Time interval : 10, 12, 15, 20sec & eeg_remove_time_in_group : 7, 9, 12, 16sec (7~80% proportion of time interval)\n",
    "    R-squared score was best when we split the dataset into 10 seconds group.\n",
    "    \n",
    "    2. BM (Body Movement) sum minutes : 1h, 1h 30m, 2h, 2h 30m, 3h\n",
    "    R-squared score was best when we set up the BM (Body Movement) sum minutes as 3 hours.     \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eeg_filepath, fitbit_folderpath, time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180):\n",
    "        # Initialize by calling parent class constructor\n",
    "        EEGProcessor.__init__(self, eeg_filepath, time_interval, eeg_remove_time_in_group)\n",
    "        FitbitProcessor.__init__(self, fitbit_folderpath, time_interval, BM_sum_minutes)\n",
    "        \n",
    "        self.eeg_filepath = eeg_filepath\n",
    "\n",
    "    # Merge EEG and Fitbit data\n",
    "    def merge_data(self):\n",
    "        # processing EEG data\n",
    "        eeg_data = pd.read_csv(self.eeg_filepath)\n",
    "        result_dfs = []\n",
    "        \n",
    "        # For all experiments in the eeg data csv file\n",
    "        for exp_id in range(3, len(eeg_data)):\n",
    "            processed_data = self.process_eeg_data(exp_id)\n",
    "            if processed_data is not None:\n",
    "                result_dfs.append(processed_data)\n",
    "                \n",
    "        if result_dfs:\n",
    "            combined_eeg = pd.concat(result_dfs)\n",
    "            combined_eeg.index = pd.to_datetime(combined_eeg.index)\n",
    "        \n",
    "        # processing Fitbit data\n",
    "        fitbit_data = self.process_fitbit_data()\n",
    "        fitbit_data.index = pd.to_datetime(fitbit_data.index)\n",
    "\n",
    "        # merging two dataframes\n",
    "        if 'combined_eeg' in locals() and not fitbit_data.empty:\n",
    "            merged_df = combined_eeg.merge(fitbit_data, left_index=True, right_index=True, how='left')\n",
    "            return merged_df\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_jm.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_JM\")\n",
    "\n",
    "final_jm = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_yh.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_YH\")\n",
    "\n",
    "final_yh = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SJ\")\n",
    "\n",
    "\n",
    "final_sj = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sa.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SA\")\n",
    "\n",
    "\n",
    "final_sa = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_bs.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_BS\")\n",
    "\n",
    "\n",
    "final_bs = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_mj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_MJ\")\n",
    "\n",
    "\n",
    "final_mj = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab846ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([final_jm, final_sj, final_bs, final_yh, final_mj, final_sa])\n",
    "dataset = dataset.sort_index()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(r'C:\\Users\\ballj\\OneDrive\\바탕 화면\\4.Feature_Engineering.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be3247",
   "metadata": {},
   "source": [
    "# 1~4 + 5.Feature Scaling + 6.Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EEGProcessor:\n",
    "     \n",
    "    # time_interval : Unified seconds with Fitbit data (ex. 10secs) \n",
    "    # remove_time_in_group : the criteria of processing error values in each time_interval group (ex. 7secs)\n",
    "    \n",
    "    def __init__(self, file_path, time_interval, remove_time_in_group):\n",
    "        self.time_interval = time_interval\n",
    "        self.remove_time_in_group = remove_time_in_group\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        self.EEG_report = pd.read_csv(file_path)\n",
    "\n",
    "    # List to dataframe (ex. brain waves)\n",
    "    def parse_raw_data(self, dataframe, col_name):\n",
    "        col_str = dataframe.iloc[0][col_name]\n",
    "        col_str = col_str.strip('[]')\n",
    "        col_list = [float(val) for val in col_str.split(',')]  # cause list is divided by comma\n",
    "        col_data = pd.DataFrame({col_name: col_list})\n",
    "        return col_data\n",
    "\n",
    "    # Experiment time calculating function\n",
    "    def time_difference(self, dataframe, start_time_col, finish_time_col):\n",
    "        start_time = datetime.strptime(dataframe.iloc[0][start_time_col], '%Y-%m-%d %H:%M:%S')\n",
    "        finish_time = datetime.strptime(dataframe.iloc[0][finish_time_col], '%Y-%m-%d %0H:%M:%S')\n",
    "\n",
    "        # time difference between two datatime objects\n",
    "        time_difference = (finish_time - start_time).total_seconds()\n",
    "        return time_difference\n",
    "    \n",
    "    # Comparing the experimental initial recognition error period and delete the part to be deleted\n",
    "    def count_initial_same_values(self, series):\n",
    "        initial_value = series.iloc[0]\n",
    "        count = 0\n",
    "        for value in series:\n",
    "            if value == initial_value:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        return count\n",
    "    \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_start_time_trash_sec(self, start_time):\n",
    "        # Plus 1 min and delete second in input time\n",
    "        rounded_time = start_time + timedelta(minutes=1) - timedelta(seconds=start_time.second)\n",
    "        time_difference = (rounded_time - start_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        # use the seconds over remove_time_in_group seconds\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "        \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_finish_time_trash_sec(self, finish_time):\n",
    "        # delete second in input time\n",
    "        rounded_time = finish_time - timedelta(seconds=finish_time.second)\n",
    "        time_difference = (finish_time - rounded_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "\n",
    "    # Rounding time to nearest time which can divided by time interval\n",
    "    def nearest_time_rounding(self, time):\n",
    "        seconds = time.second\n",
    "        # For example, find nearest value in 0, 10, 20, 30, 40, 50sec\n",
    "        time_points = [time_point for time_point in range(0,60, self.time_interval)]\n",
    "        nearest = min(time_points, key=lambda time_point: abs(time_point - seconds))\n",
    "        \n",
    "        if nearest == time_points[-1] and seconds >= (time_points[-1] + self.remove_time_in_group):\n",
    "            rounded_time = time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n",
    "        else:\n",
    "            rounded_time = time.replace(second=nearest, microsecond=0)\n",
    "\n",
    "        return rounded_time\n",
    "\n",
    "    # Make the same end time\n",
    "    def align_end_time(self, dataframe_1, dataframe_2):\n",
    "        if dataframe_1.index[-1] > dataframe_2.index[-1]:\n",
    "            dataframe_1 = dataframe_1[dataframe_1.index <= dataframe_2.index[-1]]\n",
    "\n",
    "        elif dataframe_1.index[-1] < dataframe_2.index[-1]:\n",
    "            dataframe_2 = dataframe_2[dataframe_2.index <= dataframe_1.index[-1]]\n",
    "\n",
    "        else: \n",
    "            pass # if two dataframe's endtime is same\n",
    "\n",
    "        return dataframe_1, dataframe_2\n",
    "\n",
    "    # Adjust start time and end time processing\n",
    "    # start time processing -> process_type : 0 , finish time processing -> process_type : -1\n",
    "    # start time processing -> process_start_time_trash_sec func , finish time processing -> process_finish_time_trash_sec func    \n",
    "    def adjust_time_index(self, process_type, dataframe, func):\n",
    "        remainder = func(dataframe.index[process_type])\n",
    "        \n",
    "        # the last data only shows one original data, so processing this problem\n",
    "        one_sec = timedelta(seconds=1)\n",
    "\n",
    "        if remainder == False:\n",
    "            # change time to nearest (Start time processing)\n",
    "            if process_type == 0 :\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type])\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "            \n",
    "            # change time to nearest (Finish time processing)\n",
    "            else:\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type]) - one_sec\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "        \n",
    "        # if remainder is under remove_time_in_group, just remove\n",
    "        else:\n",
    "            cutting_time = timedelta(seconds=remainder)\n",
    "            # Start time processing\n",
    "            if process_type == 0:\n",
    "                dataframe = dataframe[dataframe.index >= dataframe.index[process_type] + cutting_time]\n",
    "            \n",
    "            # Finish time processing\n",
    "            # make the seconds like 9, 19, 29...\n",
    "            else:\n",
    "                dataframe = dataframe[dataframe.index <= dataframe.index[-1] - cutting_time - one_sec]\n",
    "\n",
    "        return dataframe\n",
    "    \n",
    "    # Removing error values in group (brain waves and attention score)\n",
    "    def check_invalid_values(self, group):\n",
    "        # find error data length in brain wave\n",
    "        alpha_invalid_series = group['α_wave_raw_data'].diff().eq(0)\n",
    "        alpha_invalid_timestamps = group.index[alpha_invalid_series].tolist()\n",
    "\n",
    "        # find error data length in attention_raw_data\n",
    "        attention_invalid_series = group['attention_raw_data'] == 0\n",
    "        attention_invalid_timestamps = group.index[attention_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        alpha_invalid = has_long_invalid_duration(alpha_invalid_timestamps)\n",
    "        attention_invalid = has_long_invalid_duration(attention_invalid_timestamps)\n",
    "        \n",
    "        # make error values to missing values\n",
    "        if alpha_invalid or attention_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            valid_conditions = (\n",
    "                (group['α_wave_raw_data'].diff() != 0) & \n",
    "                (group['β_wave_raw_data'].diff() != 0) & \n",
    "                (group['θ_wave_raw_data'].diff() != 0) & \n",
    "                (group['δ_wave_raw_data'].diff() != 0) & \n",
    "                (group['γ_wave_raw_data'].diff() != 0) & \n",
    "                (group['attention_raw_data'] != 0)\n",
    "            )\n",
    "            return group[valid_conditions].mean()\n",
    "\n",
    "    # Removing error values in group (hr)\n",
    "    def check_invalid_values_other(self, group):\n",
    "        # find error data length in hr\n",
    "        hr_invalid_series = group['hr_raw_data'] == 0\n",
    "        hr_invalid_timestamps = group.index[hr_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        hr_invalid = has_long_invalid_duration(hr_invalid_timestamps)\n",
    "\n",
    "        # make error values to missing values\n",
    "        if hr_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            group = group[(group['hr_raw_data'] != 0)]\n",
    "            return group.mean()\n",
    "    \n",
    "    # Process EEG data\n",
    "    def process_eeg_data(self, experiment_id):\n",
    "        if experiment_id not in self.EEG_report.index:\n",
    "            return None\n",
    "\n",
    "        # all experiments in one df\n",
    "        EEG_report_sample = self.EEG_report.loc[[experiment_id],:]\n",
    "\n",
    "        # one dataframe for one column\n",
    "        cols = ['α_wave_raw_data', 'β_wave_raw_data', 'θ_wave_raw_data', 'δ_wave_raw_data', 'γ_wave_raw_data', 'attention_raw_data', 'hrv_raw_data', 'hr_raw_data', 'coherence_flag_raw_data']\n",
    "        parsed_dfs = [self.parse_raw_data(EEG_report_sample, col) for col in cols]\n",
    "\n",
    "        # calculate two interval second because there's two type of time interval in EEG data\n",
    "        interval_sec = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[0])\n",
    "        interval_sec_other = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[6])\n",
    "\n",
    "        # make two merged dataframe\n",
    "        merged_df = parsed_dfs[0].join(parsed_dfs[1:6])\n",
    "        merged_df_other = parsed_dfs[6].join(parsed_dfs[7:])\n",
    "\n",
    "        # experiment start time\n",
    "        start_time = datetime.strptime(EEG_report_sample.iloc[0]['meditation_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # change index to time index based on interval second\n",
    "        interval_sec, interval_sec_other = timedelta(seconds=round(interval_sec,2)), timedelta(seconds=round(interval_sec_other,2))\n",
    "        merged_df['time'] = [start_time + time * interval_sec for time in range(len(merged_df))]\n",
    "        merged_df_other['time'] = [start_time + time * interval_sec_other for time in range(len(merged_df_other))]\n",
    "        merged_df, merged_df_other = merged_df.set_index('time'), merged_df_other.set_index('time')\n",
    "        \n",
    "        # comparing the inital experiment error time\n",
    "        counts = [self.count_initial_same_values(merged_df[col]) for col in cols[:6]] + [self.count_initial_same_values(merged_df_other['hr_raw_data'])]\n",
    "        initial_error_times = [counts[error] * interval_sec.total_seconds() if error != 6 else counts[error] * interval_sec_other.total_seconds() for error in range(7)]\n",
    "        initial_error_time = timedelta(seconds=max(initial_error_times))\n",
    "\n",
    "        # dataset start time\n",
    "        real_start_time = start_time + initial_error_time\n",
    "        merged_df, merged_df_other = merged_df[merged_df.index > real_start_time], merged_df_other[merged_df_other.index > real_start_time]\n",
    "        merged_df.index, merged_df_other.index = merged_df.index.round('S'), merged_df_other.index.round('S')\n",
    "\n",
    "        # make the experiment end time same\n",
    "        merged_df, merged_df_other = self.align_end_time(merged_df, merged_df_other)\n",
    "\n",
    "        # start time process -> i : 0 , finish time process -> i : -1\n",
    "        # start time process -> process_start_time_trash_sec func , finish time process -> process_finish_time_trash_sec func\n",
    "        merged_df = self.adjust_time_index(0, merged_df, self.process_start_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(0, merged_df_other, self.process_start_time_trash_sec)\n",
    "        merged_df = self.adjust_time_index(-1, merged_df, self.process_finish_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(-1, merged_df_other, self.process_finish_time_trash_sec)\n",
    "\n",
    "        # grouping\n",
    "        grouped = merged_df.groupby(merged_df.index.floor(self.time_interval_str))\n",
    "        grouped_other = merged_df_other.groupby(merged_df_other.index.floor(self.time_interval_str))\n",
    "\n",
    "        result = grouped.apply(self.check_invalid_values)\n",
    "        result_other = grouped_other.apply(self.check_invalid_values_other)\n",
    "\n",
    "        # final EEG dataset including β/θ SP ratio\n",
    "        EEG_data_per_time_interval = result.merge(result_other, left_index=True, right_index=True)\n",
    "        EEG_data_per_time_interval['β/θ SP'] = EEG_data_per_time_interval['β_wave_raw_data'] / EEG_data_per_time_interval['θ_wave_raw_data']\n",
    "        \n",
    "        EEG_data_per_time_interval = EEG_data_per_time_interval.rename(columns={\n",
    "            'α_wave_raw_data':'alpha_wave',\n",
    "            'β_wave_raw_data':'beta_wave',\n",
    "            'θ_wave_raw_data':'theta_wave',\n",
    "            'δ_wave_raw_data':'delta_wave',\n",
    "            'γ_wave_raw_data':'gamma_wave',\n",
    "            'attention_raw_data' : 'attention',\n",
    "            'hrv_raw_data' : 'hrv',\n",
    "            'hr_raw_data' : 'hr',\n",
    "            'coherence_flag_raw_data' : 'coherence',\n",
    "            'β/θ SP' : 'SP ratio'\n",
    "        })\n",
    "\n",
    "        return EEG_data_per_time_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "class FitbitProcessor:\n",
    "    '''\n",
    "    time_interval : Unified seconds with EEG dataset (ex.10secs) \n",
    "    BM_sum_minutes\n",
    "    Body Movement feature was meaningless because the experiment was conducted usually while sitting on the chair.\n",
    "    So, created a new body movement feature as accumulated Body Movement value from previous time.\n",
    "    And, the previous time is the BM_sum_minutes variable. (ex. 3 hours)\n",
    "    '''\n",
    "    def __init__(self, folder_path, time_interval, BM_sum_minutes):\n",
    "        self.folder_path = folder_path\n",
    "        self.BM_sum_minutes = BM_sum_minutes\n",
    "        self.BM_sum_minutes_str = f'{BM_sum_minutes}T'\n",
    "        self.time_interval = time_interval\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        folder_patterns = [\n",
    "            \"Active Zone Minutes (AZM)/*\",\n",
    "            \"Sleep Score/*\",\n",
    "            \"Stress Journal/CEDA*\",\n",
    "            \"Temperature/Wrist Temperature - *\"\n",
    "        ]\n",
    "        self.things_path = [glob.glob(f\"{folder_path}/{pattern}\") for pattern in folder_patterns]\n",
    "        self.things_path = [item for sublist in self.things_path for item in sublist]\n",
    "        self.wt_count = len(glob.glob(f\"{folder_path}/Temperature/Wrist Temperature - *\"))\n",
    "        self.azm_count = len(glob.glob(f\"{folder_path}/Active Zone Minutes (AZM)/*\"))\n",
    "        self.sleep_count = len(glob.glob(f\"{folder_path}/Sleep Score/*\"))\n",
    "        self.eda_count = len(glob.glob(f\"{folder_path}/Stress Journal/CEDA*\"))\n",
    "        # original fitbit dataset's time interval is 1 min\n",
    "        self.original_interval = 60\n",
    "        self.num_timestamps = self.original_interval // self.time_interval\n",
    "        self.half_point = self.num_timestamps // 2\n",
    "    \n",
    "    def read_filtered_csv(self, path, columns):\n",
    "        name = pd.read_csv(path)\n",
    "        name = name[columns]\n",
    "        name[columns[0]] = pd.to_datetime(name[columns[0]])\n",
    "        return name\n",
    "    \n",
    "    def round_seconds(self, obj):\n",
    "        if obj.second % self.time_interval == 0:\n",
    "            return obj\n",
    "        else:\n",
    "            return obj - timedelta(seconds=obj.second % self.time_interval)\n",
    "    \n",
    "    def round_zero(self, datetime_obj):\n",
    "        datetime_obj = datetime_obj.replace(second=0)\n",
    "        return datetime_obj\n",
    "    \n",
    "    # Process whole fitbit data\n",
    "    def process_fitbit_data(self):\n",
    "        AZM_col = ['date_time', 'total_minutes']\n",
    "        sleep_col = ['timestamp', 'deep_sleep_in_minutes']\n",
    "        stress_col = ['timestamp', 'eda_level_real']\n",
    "        temp_col = ['recorded_time', 'temperature']\n",
    "        \n",
    "        # merge all features\n",
    "        things_col = [AZM_col] * self.azm_count + [sleep_col] * self.sleep_count + [stress_col] * self.eda_count + [temp_col] * self.wt_count\n",
    "        things = [self.read_filtered_csv(path, col) for path, col in zip(self.things_path, things_col)]\n",
    "\n",
    "        # if there's no wrist temperature\n",
    "        if self.wt_count == 0:\n",
    "            # if there's no eda data\n",
    "            # there was no eda in two subjects' fitbit data\n",
    "            if self.eda_count == 0:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep])\n",
    "                \n",
    "                # make final dataframe\n",
    "                df = self.create_final_df([azm, sleep], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                df = df.assign(eda=np.nan)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            else:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda])\n",
    "\n",
    "                df = self.create_final_df([azm, sleep, eda], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                return df\n",
    "            \n",
    "        # if there's no Active Zone Minutes data\n",
    "        elif self.azm_count == 0:\n",
    "            sleep = self.process_sleep(things[self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(BM=np.nan)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # if there's no sleep data\n",
    "        elif self.sleep_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(sleep=np.nan)\n",
    "\n",
    "            return df            \n",
    "        \n",
    "        # if there's no eda data\n",
    "        elif self.eda_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(eda=np.nan)\n",
    "\n",
    "            return df\n",
    "            \n",
    "        # if there's no error in data file\n",
    "        else:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "\n",
    "            return df\n",
    "    \n",
    "    # Process Active Zone Minutes data\n",
    "    def process_azm(self, azm):\n",
    "\n",
    "        azm = pd.concat(azm, axis=0)\n",
    "        azm = azm.rename(columns={'date_time':'timestamp', 'total_minutes':'BM'})\n",
    "        time_list = []\n",
    "        body_movement = []\n",
    "\n",
    "        for time in azm['timestamp']:\n",
    "            start_timestamp = time - timedelta(seconds=(self.half_point * self.time_interval))\n",
    "\n",
    "            for number in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (number*self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "        for bm in azm['BM']:\n",
    "            for _ in range(self.num_timestamps):\n",
    "                body_movement.append(bm)\n",
    "\n",
    "        azm_list = {'timestamp': time_list, 'BM': body_movement}\n",
    "        azm_final = pd.DataFrame(azm_list)\n",
    "\n",
    "        azm_final['timestamp'] = pd.to_datetime(azm_final['timestamp'])\n",
    "        azm_final.set_index('timestamp', inplace=True)\n",
    "        azm_final = azm_final.resample(self.time_interval_str).asfreq().fillna(0)\n",
    "        azm_final['new_BM'] = azm_final['BM'].rolling(self.BM_sum_minutes_str, closed='right').sum()\n",
    "        azm_final = azm_final.drop(['BM'], axis=1)\n",
    "        azm_final = azm_final.rename(columns={'new_BM':'BM'})\n",
    "        azm_final = azm_final.astype({'BM':'int'})\n",
    "        azm_final.reset_index(inplace=True)\n",
    "        \n",
    "        return azm_final\n",
    "    \n",
    "    # Process sleep data (Deep sleep in minutes)\n",
    "    def process_sleep(self, sleep):\n",
    "        sleep = pd.concat(sleep, axis=0)\n",
    "        sleep['timestamp'] = [\n",
    "                self.round_zero(datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S')) for time in sleep['timestamp']\n",
    "            ]\n",
    "            \n",
    "        sleep = sleep.rename(columns={'deep_sleep_in_minutes':'sleep'})\n",
    "        return sleep\n",
    "    \n",
    "    # Process eda data\n",
    "    def process_eda(self, eda):\n",
    "        eda = pd.concat(eda, axis=0)\n",
    "        eda['timestamp'] = [\n",
    "            self.round_seconds(\n",
    "                datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=4)\n",
    "            ) for time in eda['timestamp']]\n",
    "\n",
    "        # EDA Bilinear Interpolation\n",
    "        time_list = []\n",
    "        eda_list = []\n",
    "\n",
    "        for time in range(1, len(eda['timestamp']) - 1):\n",
    "            start_timestamp = eda.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "\n",
    "            for num in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "                if num < self.half_point:\n",
    "                    weight = (self.half_point - num) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] - ((eda.iloc[time, 1] - eda.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                elif num == self.half_point:\n",
    "                    value = eda.iloc[time,1]\n",
    "\n",
    "                else:\n",
    "                    weight = (num - self.half_point) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] + ((eda.iloc[time + 1, 1] - eda.iloc[time, 1]) * weight)\n",
    "\n",
    "                eda_list.append(round(value, 2))\n",
    "\n",
    "        eda_list = {'timestamp': time_list, 'eda': eda_list}\n",
    "        eda_final = pd.DataFrame(eda_list)\n",
    "\n",
    "        return eda_final\n",
    "    \n",
    "    # Process temperature data\n",
    "    def process_temperature(self, temperature):\n",
    "        #Temperature\n",
    "        temp = pd.concat(temperature, axis=0)\n",
    "        temp = temp.rename(columns={'recorded_time':'timestamp'})\n",
    "\n",
    "        # Temperature Bilinear Interpolation\n",
    "        time_list = []\n",
    "        temp_list = []\n",
    "\n",
    "        for time in range(1, len(temp['timestamp']) - 1):\n",
    "            if self.half_point % 2 != 0:\n",
    "                start_timestamp = temp.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "                for num in range(self.num_timestamps):\n",
    "                    new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                    time_list.append(new_timestamp)\n",
    "\n",
    "                    if num < self.half_point:\n",
    "                        weight = (self.half_point - num) / self.num_timestamps \n",
    "                        value = temp.iloc[time, 1] - ((temp.iloc[time, 1] - temp.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                    elif num == self.half_point:\n",
    "                        value = temp.iloc[time,1]\n",
    "\n",
    "                    else:\n",
    "                        weight = (num - self.half_point) / self.num_timestamps\n",
    "                        value = temp.iloc[time, 1] + ((temp.iloc[time + 1, 1] - temp.iloc[time, 1]) * weight)\n",
    "\n",
    "                    temp_list.append(round(value, 6))\n",
    "\n",
    "        temp_list = {'timestamp': time_list, 'temperature': temp_list}\n",
    "        temp_final = pd.DataFrame(temp_list)\n",
    "\n",
    "        return temp_final\n",
    "    \n",
    "    # find minimum and maximum time of whole feature\n",
    "    def find_time_bounds(self, dataframes):\n",
    "        min_times = []\n",
    "        max_times = []\n",
    "        \n",
    "        for df in dataframes:\n",
    "            if not df.empty:\n",
    "                min_times.append(df['timestamp'].min())\n",
    "                max_times.append(df['timestamp'].max())\n",
    "                \n",
    "        if not min_times or not max_times:\n",
    "            Min_Time = pd.Timestamp.now(tz='UTC')\n",
    "            Max_Time = pd.Timestamp.now(tz='UTC')\n",
    "        else:\n",
    "            Min_Time = min(min_times)\n",
    "            Max_Time = max(max_times)\n",
    "\n",
    "        return Min_Time, Max_Time\n",
    "\n",
    "    # create dataframe from Min_time to Max_time\n",
    "    def create_final_df(self, datasets, Min_Time, Max_Time):\n",
    "        fitbit = pd.date_range(start=Min_Time, end=Max_Time, freq=self.time_interval_str, name='timestamp')\n",
    "        fitbit = pd.DataFrame(fitbit)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            fitbit = pd.merge(fitbit, dataset, how='outer', on='timestamp')\n",
    "            \n",
    "        fitbit['BM'] = fitbit['BM'].fillna(0)\n",
    "        fitbit['sleep'] = fitbit['sleep'].fillna(method='ffill')\n",
    "\n",
    "        return fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger(EEGProcessor, FitbitProcessor):\n",
    "    '''\n",
    "    We've tested some hyperparameters, and \"time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180\" have shown the best R-squared score.\n",
    "    \n",
    "    [Tested hyperparameters]\n",
    "    1. Time interval : 10, 12, 15, 20sec & eeg_remove_time_in_group : 7, 9, 12, 16sec (7~80% proportion of time interval)\n",
    "    R-squared score was best when we split the dataset into 10 seconds group.\n",
    "    \n",
    "    2. BM (Body Movement) sum minutes : 1h, 1h 30m, 2h, 2h 30m, 3h\n",
    "    R-squared score was best when we set up the BM (Body Movement) sum minutes as 3 hours.     \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eeg_filepath, fitbit_folderpath, time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180):\n",
    "        # Initialize by calling parent class constructor\n",
    "        EEGProcessor.__init__(self, eeg_filepath, time_interval, eeg_remove_time_in_group)\n",
    "        FitbitProcessor.__init__(self, fitbit_folderpath, time_interval, BM_sum_minutes)\n",
    "        \n",
    "        self.eeg_filepath = eeg_filepath\n",
    "\n",
    "    # Merge EEG and Fitbit data\n",
    "    def merge_data(self):\n",
    "        # processing EEG data\n",
    "        eeg_data = pd.read_csv(self.eeg_filepath)\n",
    "        result_dfs = []\n",
    "        \n",
    "        # For all experiments in the eeg data csv file\n",
    "        for exp_id in range(3, len(eeg_data)):\n",
    "            processed_data = self.process_eeg_data(exp_id)\n",
    "            if processed_data is not None:\n",
    "                result_dfs.append(processed_data)\n",
    "                \n",
    "        if result_dfs:\n",
    "            combined_eeg = pd.concat(result_dfs)\n",
    "            combined_eeg.index = pd.to_datetime(combined_eeg.index)\n",
    "        \n",
    "        # processing Fitbit data\n",
    "        fitbit_data = self.process_fitbit_data()\n",
    "        fitbit_data.index = pd.to_datetime(fitbit_data.index)\n",
    "\n",
    "        # merging two dataframes\n",
    "        if 'combined_eeg' in locals() and not fitbit_data.empty:\n",
    "            merged_df = combined_eeg.merge(fitbit_data, left_index=True, right_index=True, how='left')\n",
    "            return merged_df\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_jm.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_JM\")\n",
    "\n",
    "final_jm = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_yh.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_YH\")\n",
    "\n",
    "final_yh = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SJ\")\n",
    "\n",
    "\n",
    "final_sj = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sa.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SA\")\n",
    "\n",
    "\n",
    "final_sa = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_bs.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_BS\")\n",
    "\n",
    "\n",
    "final_bs = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_mj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_MJ\")\n",
    "\n",
    "\n",
    "final_mj = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0155c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([final_jm, final_sj, final_bs, final_yh, final_mj, final_sa])\n",
    "dataset = dataset.sort_index()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00eeb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "coherence_original = dataset_original['coherence'].copy()\n",
    "# Temp - Standard Scaler & Else - Min-Max Scaling\n",
    "features_to_minmax = dataset_original.drop(['temperature', 'SP ratio', 'coherence'], axis=1)\n",
    "feature_to_standardize = dataset_original[['temperature']]\n",
    "feature_to_minmax_0_1 = dataset_original[['SP ratio']]\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler_0_1 = MinMaxScaler(feature_range=(0, 1))\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "features_to_minmax_scaled = minmax_scaler.fit_transform(features_to_minmax)\n",
    "feature_to_standardize_scaled = standard_scaler.fit_transform(feature_to_standardize)\n",
    "feature_to_minmax_0_1_scaled = minmax_scaler_0_1.fit_transform(feature_to_minmax_0_1)\n",
    "\n",
    "# convert to dataframe\n",
    "features_to_minmax_scaled_df = pd.DataFrame(features_to_minmax_scaled, \n",
    "                                            index=dataset_original.index, \n",
    "                                            columns=features_to_minmax.columns)\n",
    "feature_to_standardize_scaled_df = pd.DataFrame(feature_to_standardize_scaled, \n",
    "                                                index=dataset_original.index, \n",
    "                                                columns=['temperature'])\n",
    "feature_to_minmax_0_1_scaled_df = pd.DataFrame(feature_to_minmax_0_1_scaled, \n",
    "                                                index=dataset_original.index, \n",
    "                                                columns=['SP ratio'])\n",
    "\n",
    "dataset_original.update(features_to_minmax_scaled_df)\n",
    "dataset_original.update(feature_to_standardize_scaled_df)\n",
    "dataset_original.update(feature_to_minmax_0_1_scaled_df)\n",
    "\n",
    "dataset_original['coherence'] = coherence_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84803c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Temperature Imputation (Random Forest Imputation)\n",
    "temperature_na1 = dataset_original[dataset_original['temperature'].isna() & dataset_original.drop(columns='temperature').notna().all(axis=1)]\n",
    "temperature_na1 = temperature_na1.reset_index()\n",
    "temperature_na1 = temperature_na1.drop(['index'], axis=1)\n",
    "\n",
    "full_dataset1 = dataset_original.dropna()\n",
    "full_dataset1 = full_dataset1.reset_index()\n",
    "full_dataset1 = full_dataset1.drop(['index'], axis=1)\n",
    "\n",
    "dataset_temp = pd.concat([full_dataset1, temperature_na1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af126acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using Random Forest\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Split the dataset into two parts: with and without missing 'temperature' values\n",
    "dataset_with_temperature = dataset_temp.dropna(subset=['temperature'])\n",
    "dataset_without_temperature = dataset_temp[dataset_temp['temperature'].isnull()]\n",
    "\n",
    "# Train the model on the rows with no missing temperature values\n",
    "regressor.fit(dataset_with_temperature.drop('temperature', axis=1), dataset_with_temperature['temperature'])\n",
    "\n",
    "# Predict the missing temperature values\n",
    "predicted_temperatures = regressor.predict(dataset_without_temperature.drop('temperature', axis=1))\n",
    "\n",
    "# Fill in the missing values in the original dataframe\n",
    "dataset_temp.loc[dataset_temp['temperature'].isnull(), 'temperature'] = predicted_temperatures\n",
    "\n",
    "dataset_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d374cd0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "eda_na = dataset_original[dataset_original['eda'].isna() & dataset_original.drop(columns='eda').notna().all(axis=1)]\n",
    "eda_na = eda_na.reset_index()\n",
    "eda_na = eda_na.drop(['index'], axis=1)\n",
    "\n",
    "dataset_eda = pd.concat([full_dataset1, eda_na])\n",
    "\n",
    "# Impute missing values using Random Forest\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Split the dataset into two parts: with and without missing 'temperature' values\n",
    "dataset_with_eda = dataset_eda.dropna(subset=['eda'])\n",
    "dataset_without_eda = dataset_eda[dataset_eda['eda'].isnull()]\n",
    "\n",
    "# Train the model on the rows with no missing temperature values\n",
    "regressor.fit(dataset_with_eda.drop('eda', axis=1), dataset_with_eda['eda'])\n",
    "\n",
    "# Predict the missing temperature values\n",
    "predicted_edas = regressor.predict(dataset_without_eda.drop('eda', axis=1))\n",
    "\n",
    "# Fill in the missing values in the original dataframe\n",
    "dataset_eda.loc[dataset_eda['eda'].isnull(), 'eda'] = predicted_edas\n",
    "dataset_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c94631",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([dataset_temp, dataset_eda])\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba613f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(r'C:\\Users\\ballj\\OneDrive\\바탕 화면\\1~4+5.Feature_Scaling+6.Imputation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9dc7f",
   "metadata": {},
   "source": [
    "# 1~4 + 6.Imputation + 5.Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399baf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EEGProcessor:\n",
    "     \n",
    "    # time_interval : Unified seconds with Fitbit data (ex. 10secs) \n",
    "    # remove_time_in_group : the criteria of processing error values in each time_interval group (ex. 7secs)\n",
    "    \n",
    "    def __init__(self, file_path, time_interval, remove_time_in_group):\n",
    "        self.time_interval = time_interval\n",
    "        self.remove_time_in_group = remove_time_in_group\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        self.EEG_report = pd.read_csv(file_path)\n",
    "\n",
    "    # List to dataframe (ex. brain waves)\n",
    "    def parse_raw_data(self, dataframe, col_name):\n",
    "        col_str = dataframe.iloc[0][col_name]\n",
    "        col_str = col_str.strip('[]')\n",
    "        col_list = [float(val) for val in col_str.split(',')]  # cause list is divided by comma\n",
    "        col_data = pd.DataFrame({col_name: col_list})\n",
    "        return col_data\n",
    "\n",
    "    # Experiment time calculating function\n",
    "    def time_difference(self, dataframe, start_time_col, finish_time_col):\n",
    "        start_time = datetime.strptime(dataframe.iloc[0][start_time_col], '%Y-%m-%d %H:%M:%S')\n",
    "        finish_time = datetime.strptime(dataframe.iloc[0][finish_time_col], '%Y-%m-%d %0H:%M:%S')\n",
    "\n",
    "        # time difference between two datatime objects\n",
    "        time_difference = (finish_time - start_time).total_seconds()\n",
    "        return time_difference\n",
    "    \n",
    "    # Comparing the experimental initial recognition error period and delete the part to be deleted\n",
    "    def count_initial_same_values(self, series):\n",
    "        initial_value = series.iloc[0]\n",
    "        count = 0\n",
    "        for value in series:\n",
    "            if value == initial_value:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        return count\n",
    "    \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_start_time_trash_sec(self, start_time):\n",
    "        # Plus 1 min and delete second in input time\n",
    "        rounded_time = start_time + timedelta(minutes=1) - timedelta(seconds=start_time.second)\n",
    "        time_difference = (rounded_time - start_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        # use the seconds over remove_time_in_group seconds\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "        \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_finish_time_trash_sec(self, finish_time):\n",
    "        # delete second in input time\n",
    "        rounded_time = finish_time - timedelta(seconds=finish_time.second)\n",
    "        time_difference = (finish_time - rounded_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "\n",
    "    # Rounding time to nearest time which can divided by time interval\n",
    "    def nearest_time_rounding(self, time):\n",
    "        seconds = time.second\n",
    "        # For example, find nearest value in 0, 10, 20, 30, 40, 50sec\n",
    "        time_points = [time_point for time_point in range(0,60, self.time_interval)]\n",
    "        nearest = min(time_points, key=lambda time_point: abs(time_point - seconds))\n",
    "        \n",
    "        if nearest == time_points[-1] and seconds >= (time_points[-1] + self.remove_time_in_group):\n",
    "            rounded_time = time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n",
    "        else:\n",
    "            rounded_time = time.replace(second=nearest, microsecond=0)\n",
    "\n",
    "        return rounded_time\n",
    "\n",
    "    # Make the same end time\n",
    "    def align_end_time(self, dataframe_1, dataframe_2):\n",
    "        if dataframe_1.index[-1] > dataframe_2.index[-1]:\n",
    "            dataframe_1 = dataframe_1[dataframe_1.index <= dataframe_2.index[-1]]\n",
    "\n",
    "        elif dataframe_1.index[-1] < dataframe_2.index[-1]:\n",
    "            dataframe_2 = dataframe_2[dataframe_2.index <= dataframe_1.index[-1]]\n",
    "\n",
    "        else: \n",
    "            pass # if two dataframe's endtime is same\n",
    "\n",
    "        return dataframe_1, dataframe_2\n",
    "\n",
    "    # Adjust start time and end time processing\n",
    "    # start time processing -> process_type : 0 , finish time processing -> process_type : -1\n",
    "    # start time processing -> process_start_time_trash_sec func , finish time processing -> process_finish_time_trash_sec func    \n",
    "    def adjust_time_index(self, process_type, dataframe, func):\n",
    "        remainder = func(dataframe.index[process_type])\n",
    "        \n",
    "        # the last data only shows one original data, so processing this problem\n",
    "        one_sec = timedelta(seconds=1)\n",
    "\n",
    "        if remainder == False:\n",
    "            # change time to nearest (Start time processing)\n",
    "            if process_type == 0 :\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type])\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "            \n",
    "            # change time to nearest (Finish time processing)\n",
    "            else:\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type]) - one_sec\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "        \n",
    "        # if remainder is under remove_time_in_group, just remove\n",
    "        else:\n",
    "            cutting_time = timedelta(seconds=remainder)\n",
    "            # Start time processing\n",
    "            if process_type == 0:\n",
    "                dataframe = dataframe[dataframe.index >= dataframe.index[process_type] + cutting_time]\n",
    "            \n",
    "            # Finish time processing\n",
    "            # make the seconds like 9, 19, 29...\n",
    "            else:\n",
    "                dataframe = dataframe[dataframe.index <= dataframe.index[-1] - cutting_time - one_sec]\n",
    "\n",
    "        return dataframe\n",
    "    \n",
    "    # Removing error values in group (brain waves and attention score)\n",
    "    def check_invalid_values(self, group):\n",
    "        # find error data length in brain wave\n",
    "        alpha_invalid_series = group['α_wave_raw_data'].diff().eq(0)\n",
    "        alpha_invalid_timestamps = group.index[alpha_invalid_series].tolist()\n",
    "\n",
    "        # find error data length in attention_raw_data\n",
    "        attention_invalid_series = group['attention_raw_data'] == 0\n",
    "        attention_invalid_timestamps = group.index[attention_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        alpha_invalid = has_long_invalid_duration(alpha_invalid_timestamps)\n",
    "        attention_invalid = has_long_invalid_duration(attention_invalid_timestamps)\n",
    "        \n",
    "        # make error values to missing values\n",
    "        if alpha_invalid or attention_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            valid_conditions = (\n",
    "                (group['α_wave_raw_data'].diff() != 0) & \n",
    "                (group['β_wave_raw_data'].diff() != 0) & \n",
    "                (group['θ_wave_raw_data'].diff() != 0) & \n",
    "                (group['δ_wave_raw_data'].diff() != 0) & \n",
    "                (group['γ_wave_raw_data'].diff() != 0) & \n",
    "                (group['attention_raw_data'] != 0)\n",
    "            )\n",
    "            return group[valid_conditions].mean()\n",
    "\n",
    "    # Removing error values in group (hr)\n",
    "    def check_invalid_values_other(self, group):\n",
    "        # find error data length in hr\n",
    "        hr_invalid_series = group['hr_raw_data'] == 0\n",
    "        hr_invalid_timestamps = group.index[hr_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        hr_invalid = has_long_invalid_duration(hr_invalid_timestamps)\n",
    "\n",
    "        # make error values to missing values\n",
    "        if hr_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            group = group[(group['hr_raw_data'] != 0)]\n",
    "            return group.mean()\n",
    "    \n",
    "    # Process EEG data\n",
    "    def process_eeg_data(self, experiment_id):\n",
    "        if experiment_id not in self.EEG_report.index:\n",
    "            return None\n",
    "\n",
    "        # all experiments in one df\n",
    "        EEG_report_sample = self.EEG_report.loc[[experiment_id],:]\n",
    "\n",
    "        # one dataframe for one column\n",
    "        cols = ['α_wave_raw_data', 'β_wave_raw_data', 'θ_wave_raw_data', 'δ_wave_raw_data', 'γ_wave_raw_data', 'attention_raw_data', 'hrv_raw_data', 'hr_raw_data', 'coherence_flag_raw_data']\n",
    "        parsed_dfs = [self.parse_raw_data(EEG_report_sample, col) for col in cols]\n",
    "\n",
    "        # calculate two interval second because there's two type of time interval in EEG data\n",
    "        interval_sec = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[0])\n",
    "        interval_sec_other = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[6])\n",
    "\n",
    "        # make two merged dataframe\n",
    "        merged_df = parsed_dfs[0].join(parsed_dfs[1:6])\n",
    "        merged_df_other = parsed_dfs[6].join(parsed_dfs[7:])\n",
    "\n",
    "        # experiment start time\n",
    "        start_time = datetime.strptime(EEG_report_sample.iloc[0]['meditation_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # change index to time index based on interval second\n",
    "        interval_sec, interval_sec_other = timedelta(seconds=round(interval_sec,2)), timedelta(seconds=round(interval_sec_other,2))\n",
    "        merged_df['time'] = [start_time + time * interval_sec for time in range(len(merged_df))]\n",
    "        merged_df_other['time'] = [start_time + time * interval_sec_other for time in range(len(merged_df_other))]\n",
    "        merged_df, merged_df_other = merged_df.set_index('time'), merged_df_other.set_index('time')\n",
    "        \n",
    "        # comparing the inital experiment error time\n",
    "        counts = [self.count_initial_same_values(merged_df[col]) for col in cols[:6]] + [self.count_initial_same_values(merged_df_other['hr_raw_data'])]\n",
    "        initial_error_times = [counts[error] * interval_sec.total_seconds() if error != 6 else counts[error] * interval_sec_other.total_seconds() for error in range(7)]\n",
    "        initial_error_time = timedelta(seconds=max(initial_error_times))\n",
    "\n",
    "        # dataset start time\n",
    "        real_start_time = start_time + initial_error_time\n",
    "        merged_df, merged_df_other = merged_df[merged_df.index > real_start_time], merged_df_other[merged_df_other.index > real_start_time]\n",
    "        merged_df.index, merged_df_other.index = merged_df.index.round('S'), merged_df_other.index.round('S')\n",
    "\n",
    "        # make the experiment end time same\n",
    "        merged_df, merged_df_other = self.align_end_time(merged_df, merged_df_other)\n",
    "\n",
    "        # start time process -> i : 0 , finish time process -> i : -1\n",
    "        # start time process -> process_start_time_trash_sec func , finish time process -> process_finish_time_trash_sec func\n",
    "        merged_df = self.adjust_time_index(0, merged_df, self.process_start_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(0, merged_df_other, self.process_start_time_trash_sec)\n",
    "        merged_df = self.adjust_time_index(-1, merged_df, self.process_finish_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(-1, merged_df_other, self.process_finish_time_trash_sec)\n",
    "\n",
    "        # grouping\n",
    "        grouped = merged_df.groupby(merged_df.index.floor(self.time_interval_str))\n",
    "        grouped_other = merged_df_other.groupby(merged_df_other.index.floor(self.time_interval_str))\n",
    "\n",
    "        result = grouped.apply(self.check_invalid_values)\n",
    "        result_other = grouped_other.apply(self.check_invalid_values_other)\n",
    "\n",
    "        # final EEG dataset including β/θ SP ratio\n",
    "        EEG_data_per_time_interval = result.merge(result_other, left_index=True, right_index=True)\n",
    "        EEG_data_per_time_interval['β/θ SP'] = EEG_data_per_time_interval['β_wave_raw_data'] / EEG_data_per_time_interval['θ_wave_raw_data']\n",
    "        \n",
    "        EEG_data_per_time_interval = EEG_data_per_time_interval.rename(columns={\n",
    "            'α_wave_raw_data':'alpha_wave',\n",
    "            'β_wave_raw_data':'beta_wave',\n",
    "            'θ_wave_raw_data':'theta_wave',\n",
    "            'δ_wave_raw_data':'delta_wave',\n",
    "            'γ_wave_raw_data':'gamma_wave',\n",
    "            'attention_raw_data' : 'attention',\n",
    "            'hrv_raw_data' : 'hrv',\n",
    "            'hr_raw_data' : 'hr',\n",
    "            'coherence_flag_raw_data' : 'coherence',\n",
    "            'β/θ SP' : 'SP ratio'\n",
    "        })\n",
    "\n",
    "        return EEG_data_per_time_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a94947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "class FitbitProcessor:\n",
    "    '''\n",
    "    time_interval : Unified seconds with EEG dataset (ex.10secs) \n",
    "    BM_sum_minutes\n",
    "    Body Movement feature was meaningless because the experiment was conducted usually while sitting on the chair.\n",
    "    So, created a new body movement feature as accumulated Body Movement value from previous time.\n",
    "    And, the previous time is the BM_sum_minutes variable. (ex. 3 hours)\n",
    "    '''\n",
    "    def __init__(self, folder_path, time_interval, BM_sum_minutes):\n",
    "        self.folder_path = folder_path\n",
    "        self.BM_sum_minutes = BM_sum_minutes\n",
    "        self.BM_sum_minutes_str = f'{BM_sum_minutes}T'\n",
    "        self.time_interval = time_interval\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        folder_patterns = [\n",
    "            \"Active Zone Minutes (AZM)/*\",\n",
    "            \"Sleep Score/*\",\n",
    "            \"Stress Journal/CEDA*\",\n",
    "            \"Temperature/Wrist Temperature - *\"\n",
    "        ]\n",
    "        self.things_path = [glob.glob(f\"{folder_path}/{pattern}\") for pattern in folder_patterns]\n",
    "        self.things_path = [item for sublist in self.things_path for item in sublist]\n",
    "        self.wt_count = len(glob.glob(f\"{folder_path}/Temperature/Wrist Temperature - *\"))\n",
    "        self.azm_count = len(glob.glob(f\"{folder_path}/Active Zone Minutes (AZM)/*\"))\n",
    "        self.sleep_count = len(glob.glob(f\"{folder_path}/Sleep Score/*\"))\n",
    "        self.eda_count = len(glob.glob(f\"{folder_path}/Stress Journal/CEDA*\"))\n",
    "        # original fitbit dataset's time interval is 1 min\n",
    "        self.original_interval = 60\n",
    "        self.num_timestamps = self.original_interval // self.time_interval\n",
    "        self.half_point = self.num_timestamps // 2\n",
    "    \n",
    "    def read_filtered_csv(self, path, columns):\n",
    "        name = pd.read_csv(path)\n",
    "        name = name[columns]\n",
    "        name[columns[0]] = pd.to_datetime(name[columns[0]])\n",
    "        return name\n",
    "    \n",
    "    def round_seconds(self, obj):\n",
    "        if obj.second % self.time_interval == 0:\n",
    "            return obj\n",
    "        else:\n",
    "            return obj - timedelta(seconds=obj.second % self.time_interval)\n",
    "    \n",
    "    def round_zero(self, datetime_obj):\n",
    "        datetime_obj = datetime_obj.replace(second=0)\n",
    "        return datetime_obj\n",
    "    \n",
    "    # Process whole fitbit data\n",
    "    def process_fitbit_data(self):\n",
    "        AZM_col = ['date_time', 'total_minutes']\n",
    "        sleep_col = ['timestamp', 'deep_sleep_in_minutes']\n",
    "        stress_col = ['timestamp', 'eda_level_real']\n",
    "        temp_col = ['recorded_time', 'temperature']\n",
    "        \n",
    "        # merge all features\n",
    "        things_col = [AZM_col] * self.azm_count + [sleep_col] * self.sleep_count + [stress_col] * self.eda_count + [temp_col] * self.wt_count\n",
    "        things = [self.read_filtered_csv(path, col) for path, col in zip(self.things_path, things_col)]\n",
    "\n",
    "        # if there's no wrist temperature\n",
    "        if self.wt_count == 0:\n",
    "            # if there's no eda data\n",
    "            # there was no eda in two subjects' fitbit data\n",
    "            if self.eda_count == 0:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep])\n",
    "                \n",
    "                # make final dataframe\n",
    "                df = self.create_final_df([azm, sleep], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                df = df.assign(eda=np.nan)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            else:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda])\n",
    "\n",
    "                df = self.create_final_df([azm, sleep, eda], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                return df\n",
    "            \n",
    "        # if there's no Active Zone Minutes data\n",
    "        elif self.azm_count == 0:\n",
    "            sleep = self.process_sleep(things[self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(BM=np.nan)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # if there's no sleep data\n",
    "        elif self.sleep_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(sleep=np.nan)\n",
    "\n",
    "            return df            \n",
    "        \n",
    "        # if there's no eda data\n",
    "        elif self.eda_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(eda=np.nan)\n",
    "\n",
    "            return df\n",
    "            \n",
    "        # if there's no error in data file\n",
    "        else:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "\n",
    "            return df\n",
    "    \n",
    "    # Process Active Zone Minutes data\n",
    "    def process_azm(self, azm):\n",
    "\n",
    "        azm = pd.concat(azm, axis=0)\n",
    "        azm = azm.rename(columns={'date_time':'timestamp', 'total_minutes':'BM'})\n",
    "        time_list = []\n",
    "        body_movement = []\n",
    "\n",
    "        for time in azm['timestamp']:\n",
    "            start_timestamp = time - timedelta(seconds=(self.half_point * self.time_interval))\n",
    "\n",
    "            for number in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (number*self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "        for bm in azm['BM']:\n",
    "            for _ in range(self.num_timestamps):\n",
    "                body_movement.append(bm)\n",
    "\n",
    "        azm_list = {'timestamp': time_list, 'BM': body_movement}\n",
    "        azm_final = pd.DataFrame(azm_list)\n",
    "\n",
    "        azm_final['timestamp'] = pd.to_datetime(azm_final['timestamp'])\n",
    "        azm_final.set_index('timestamp', inplace=True)\n",
    "        azm_final = azm_final.resample(self.time_interval_str).asfreq().fillna(0)\n",
    "        azm_final['new_BM'] = azm_final['BM'].rolling(self.BM_sum_minutes_str, closed='right').sum()\n",
    "        azm_final = azm_final.drop(['BM'], axis=1)\n",
    "        azm_final = azm_final.rename(columns={'new_BM':'BM'})\n",
    "        azm_final = azm_final.astype({'BM':'int'})\n",
    "        azm_final.reset_index(inplace=True)\n",
    "        \n",
    "        return azm_final\n",
    "    \n",
    "    # Process sleep data (Deep sleep in minutes)\n",
    "    def process_sleep(self, sleep):\n",
    "        sleep = pd.concat(sleep, axis=0)\n",
    "        sleep['timestamp'] = [\n",
    "                self.round_zero(datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S')) for time in sleep['timestamp']\n",
    "            ]\n",
    "            \n",
    "        sleep = sleep.rename(columns={'deep_sleep_in_minutes':'sleep'})\n",
    "        return sleep\n",
    "    \n",
    "    # Process eda data\n",
    "    def process_eda(self, eda):\n",
    "        eda = pd.concat(eda, axis=0)\n",
    "        eda['timestamp'] = [\n",
    "            self.round_seconds(\n",
    "                datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=4)\n",
    "            ) for time in eda['timestamp']]\n",
    "\n",
    "        # EDA Bilinear Interpolation\n",
    "        time_list = []\n",
    "        eda_list = []\n",
    "\n",
    "        for time in range(1, len(eda['timestamp']) - 1):\n",
    "            start_timestamp = eda.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "\n",
    "            for num in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "                if num < self.half_point:\n",
    "                    weight = (self.half_point - num) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] - ((eda.iloc[time, 1] - eda.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                elif num == self.half_point:\n",
    "                    value = eda.iloc[time,1]\n",
    "\n",
    "                else:\n",
    "                    weight = (num - self.half_point) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] + ((eda.iloc[time + 1, 1] - eda.iloc[time, 1]) * weight)\n",
    "\n",
    "                eda_list.append(round(value, 2))\n",
    "\n",
    "        eda_list = {'timestamp': time_list, 'eda': eda_list}\n",
    "        eda_final = pd.DataFrame(eda_list)\n",
    "\n",
    "        return eda_final\n",
    "    \n",
    "    # Process temperature data\n",
    "    def process_temperature(self, temperature):\n",
    "        #Temperature\n",
    "        temp = pd.concat(temperature, axis=0)\n",
    "        temp = temp.rename(columns={'recorded_time':'timestamp'})\n",
    "\n",
    "        # Temperature Bilinear Interpolation\n",
    "        time_list = []\n",
    "        temp_list = []\n",
    "\n",
    "        for time in range(1, len(temp['timestamp']) - 1):\n",
    "            if self.half_point % 2 != 0:\n",
    "                start_timestamp = temp.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "                for num in range(self.num_timestamps):\n",
    "                    new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                    time_list.append(new_timestamp)\n",
    "\n",
    "                    if num < self.half_point:\n",
    "                        weight = (self.half_point - num) / self.num_timestamps \n",
    "                        value = temp.iloc[time, 1] - ((temp.iloc[time, 1] - temp.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                    elif num == self.half_point:\n",
    "                        value = temp.iloc[time,1]\n",
    "\n",
    "                    else:\n",
    "                        weight = (num - self.half_point) / self.num_timestamps\n",
    "                        value = temp.iloc[time, 1] + ((temp.iloc[time + 1, 1] - temp.iloc[time, 1]) * weight)\n",
    "\n",
    "                    temp_list.append(round(value, 6))\n",
    "\n",
    "        temp_list = {'timestamp': time_list, 'temperature': temp_list}\n",
    "        temp_final = pd.DataFrame(temp_list)\n",
    "\n",
    "        return temp_final\n",
    "    \n",
    "    # find minimum and maximum time of whole feature\n",
    "    def find_time_bounds(self, dataframes):\n",
    "        min_times = []\n",
    "        max_times = []\n",
    "        \n",
    "        for df in dataframes:\n",
    "            if not df.empty:\n",
    "                min_times.append(df['timestamp'].min())\n",
    "                max_times.append(df['timestamp'].max())\n",
    "                \n",
    "        if not min_times or not max_times:\n",
    "            Min_Time = pd.Timestamp.now(tz='UTC')\n",
    "            Max_Time = pd.Timestamp.now(tz='UTC')\n",
    "        else:\n",
    "            Min_Time = min(min_times)\n",
    "            Max_Time = max(max_times)\n",
    "\n",
    "        return Min_Time, Max_Time\n",
    "\n",
    "    # create dataframe from Min_time to Max_time\n",
    "    def create_final_df(self, datasets, Min_Time, Max_Time):\n",
    "        fitbit = pd.date_range(start=Min_Time, end=Max_Time, freq=self.time_interval_str, name='timestamp')\n",
    "        fitbit = pd.DataFrame(fitbit)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            fitbit = pd.merge(fitbit, dataset, how='outer', on='timestamp')\n",
    "            \n",
    "        fitbit['BM'] = fitbit['BM'].fillna(0)\n",
    "        fitbit['sleep'] = fitbit['sleep'].fillna(method='ffill')\n",
    "\n",
    "        return fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger(EEGProcessor, FitbitProcessor):\n",
    "    '''\n",
    "    We've tested some hyperparameters, and \"time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180\" have shown the best R-squared score.\n",
    "    \n",
    "    [Tested hyperparameters]\n",
    "    1. Time interval : 10, 12, 15, 20sec & eeg_remove_time_in_group : 7, 9, 12, 16sec (7~80% proportion of time interval)\n",
    "    R-squared score was best when we split the dataset into 10 seconds group.\n",
    "    \n",
    "    2. BM (Body Movement) sum minutes : 1h, 1h 30m, 2h, 2h 30m, 3h\n",
    "    R-squared score was best when we set up the BM (Body Movement) sum minutes as 3 hours.     \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eeg_filepath, fitbit_folderpath, time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180):\n",
    "        # Initialize by calling parent class constructor\n",
    "        EEGProcessor.__init__(self, eeg_filepath, time_interval, eeg_remove_time_in_group)\n",
    "        FitbitProcessor.__init__(self, fitbit_folderpath, time_interval, BM_sum_minutes)\n",
    "        \n",
    "        self.eeg_filepath = eeg_filepath\n",
    "\n",
    "    # Merge EEG and Fitbit data\n",
    "    def merge_data(self):\n",
    "        # processing EEG data\n",
    "        eeg_data = pd.read_csv(self.eeg_filepath)\n",
    "        result_dfs = []\n",
    "        \n",
    "        # For all experiments in the eeg data csv file\n",
    "        for exp_id in range(3, len(eeg_data)):\n",
    "            processed_data = self.process_eeg_data(exp_id)\n",
    "            if processed_data is not None:\n",
    "                result_dfs.append(processed_data)\n",
    "                \n",
    "        if result_dfs:\n",
    "            combined_eeg = pd.concat(result_dfs)\n",
    "            combined_eeg.index = pd.to_datetime(combined_eeg.index)\n",
    "        \n",
    "        # processing Fitbit data\n",
    "        fitbit_data = self.process_fitbit_data()\n",
    "        fitbit_data.index = pd.to_datetime(fitbit_data.index)\n",
    "\n",
    "        # merging two dataframes\n",
    "        if 'combined_eeg' in locals() and not fitbit_data.empty:\n",
    "            merged_df = combined_eeg.merge(fitbit_data, left_index=True, right_index=True, how='left')\n",
    "            return merged_df\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_jm.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_JM\")\n",
    "\n",
    "final_jm = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_yh.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_YH\")\n",
    "\n",
    "final_yh = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SJ\")\n",
    "\n",
    "\n",
    "final_sj = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sa.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SA\")\n",
    "\n",
    "\n",
    "final_sa = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_bs.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_BS\")\n",
    "\n",
    "\n",
    "final_bs = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_mj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_MJ\")\n",
    "\n",
    "\n",
    "final_mj = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d2136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([final_jm, final_sj, final_bs, final_yh, final_mj, final_sa])\n",
    "dataset = dataset.sort_index()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Temperature Imputation (Random Forest Imputation)\n",
    "temperature_na1 = dataset_original[dataset_original['temperature'].isna() & dataset_original.drop(columns='temperature').notna().all(axis=1)]\n",
    "temperature_na1 = temperature_na1.reset_index()\n",
    "temperature_na1 = temperature_na1.drop(['index'], axis=1)\n",
    "\n",
    "full_dataset1 = dataset_original.dropna()\n",
    "full_dataset1 = full_dataset1.reset_index()\n",
    "full_dataset1 = full_dataset1.drop(['index'], axis=1)\n",
    "\n",
    "dataset_temp = pd.concat([full_dataset1, temperature_na1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using Random Forest\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Split the dataset into two parts: with and without missing 'temperature' values\n",
    "dataset_with_temperature = dataset_temp.dropna(subset=['temperature'])\n",
    "dataset_without_temperature = dataset_temp[dataset_temp['temperature'].isnull()]\n",
    "\n",
    "# Train the model on the rows with no missing temperature values\n",
    "regressor.fit(dataset_with_temperature.drop('temperature', axis=1), dataset_with_temperature['temperature'])\n",
    "\n",
    "# Predict the missing temperature values\n",
    "predicted_temperatures = regressor.predict(dataset_without_temperature.drop('temperature', axis=1))\n",
    "\n",
    "# Fill in the missing values in the original dataframe\n",
    "dataset_temp.loc[dataset_temp['temperature'].isnull(), 'temperature'] = predicted_temperatures\n",
    "\n",
    "dataset_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd398a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "eda_na = dataset_original[dataset_original['eda'].isna() & dataset_original.drop(columns='eda').notna().all(axis=1)]\n",
    "eda_na = eda_na.reset_index()\n",
    "eda_na = eda_na.drop(['index'], axis=1)\n",
    "\n",
    "dataset_eda = pd.concat([full_dataset1, eda_na])\n",
    "\n",
    "# Impute missing values using Random Forest\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Split the dataset into two parts: with and without missing 'temperature' values\n",
    "dataset_with_eda = dataset_eda.dropna(subset=['eda'])\n",
    "dataset_without_eda = dataset_eda[dataset_eda['eda'].isnull()]\n",
    "\n",
    "# Train the model on the rows with no missing temperature values\n",
    "regressor.fit(dataset_with_eda.drop('eda', axis=1), dataset_with_eda['eda'])\n",
    "\n",
    "# Predict the missing temperature values\n",
    "predicted_edas = regressor.predict(dataset_without_eda.drop('eda', axis=1))\n",
    "\n",
    "# Fill in the missing values in the original dataframe\n",
    "dataset_eda.loc[dataset_eda['eda'].isnull(), 'eda'] = predicted_edas\n",
    "dataset_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82900840",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([dataset_temp, dataset_eda])\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "coherence_original = merged_df['coherence'].copy()\n",
    "# Temp - Standard Scaler & Else - Min-Max Scaling\n",
    "features_to_minmax = merged_df.drop(['temperature', 'SP ratio', 'coherence'], axis=1)\n",
    "feature_to_standardize = merged_df[['temperature']]\n",
    "feature_to_minmax_0_1 = merged_df[['SP ratio']]\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler_0_1 = MinMaxScaler(feature_range=(0, 1))\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "features_to_minmax_scaled = minmax_scaler.fit_transform(features_to_minmax)\n",
    "feature_to_standardize_scaled = standard_scaler.fit_transform(feature_to_standardize)\n",
    "feature_to_minmax_0_1_scaled = minmax_scaler_0_1.fit_transform(feature_to_minmax_0_1)\n",
    "\n",
    "# convert to dataframe\n",
    "features_to_minmax_scaled_df = pd.DataFrame(features_to_minmax_scaled, \n",
    "                                            index=merged_df.index, \n",
    "                                            columns=features_to_minmax.columns)\n",
    "feature_to_standardize_scaled_df = pd.DataFrame(feature_to_standardize_scaled, \n",
    "                                                index=merged_df.index, \n",
    "                                                columns=['temperature'])\n",
    "feature_to_minmax_0_1_scaled_df = pd.DataFrame(feature_to_minmax_0_1_scaled, \n",
    "                                                index=merged_df.index, \n",
    "                                                columns=['SP ratio'])\n",
    "\n",
    "merged_df.update(features_to_minmax_scaled_df)\n",
    "merged_df.update(feature_to_standardize_scaled_df)\n",
    "merged_df.update(feature_to_minmax_0_1_scaled_df)\n",
    "\n",
    "merged_df['coherence'] = coherence_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(r'C:\\Users\\ballj\\OneDrive\\바탕 화면\\1~4+6.Imputation+5.Feature_Scaling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28e583",
   "metadata": {},
   "source": [
    "# 1~4 + 6.Imputation + 5.Feature Scaling (SP ratio 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EEGProcessor:\n",
    "     \n",
    "    # time_interval : Unified seconds with Fitbit data (ex. 10secs) \n",
    "    # remove_time_in_group : the criteria of processing error values in each time_interval group (ex. 7secs)\n",
    "    \n",
    "    def __init__(self, file_path, time_interval, remove_time_in_group):\n",
    "        self.time_interval = time_interval\n",
    "        self.remove_time_in_group = remove_time_in_group\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        self.EEG_report = pd.read_csv(file_path)\n",
    "\n",
    "    # List to dataframe (ex. brain waves)\n",
    "    def parse_raw_data(self, dataframe, col_name):\n",
    "        col_str = dataframe.iloc[0][col_name]\n",
    "        col_str = col_str.strip('[]')\n",
    "        col_list = [float(val) for val in col_str.split(',')]  # cause list is divided by comma\n",
    "        col_data = pd.DataFrame({col_name: col_list})\n",
    "        return col_data\n",
    "\n",
    "    # Experiment time calculating function\n",
    "    def time_difference(self, dataframe, start_time_col, finish_time_col):\n",
    "        start_time = datetime.strptime(dataframe.iloc[0][start_time_col], '%Y-%m-%d %H:%M:%S')\n",
    "        finish_time = datetime.strptime(dataframe.iloc[0][finish_time_col], '%Y-%m-%d %0H:%M:%S')\n",
    "\n",
    "        # time difference between two datatime objects\n",
    "        time_difference = (finish_time - start_time).total_seconds()\n",
    "        return time_difference\n",
    "    \n",
    "    # Comparing the experimental initial recognition error period and delete the part to be deleted\n",
    "    def count_initial_same_values(self, series):\n",
    "        initial_value = series.iloc[0]\n",
    "        count = 0\n",
    "        for value in series:\n",
    "            if value == initial_value:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        return count\n",
    "    \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_start_time_trash_sec(self, start_time):\n",
    "        # Plus 1 min and delete second in input time\n",
    "        rounded_time = start_time + timedelta(minutes=1) - timedelta(seconds=start_time.second)\n",
    "        time_difference = (rounded_time - start_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        # use the seconds over remove_time_in_group seconds\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "        \n",
    "    # Processing of values that are not exactly divided into front and back\n",
    "    def process_finish_time_trash_sec(self, finish_time):\n",
    "        # delete second in input time\n",
    "        rounded_time = finish_time - timedelta(seconds=finish_time.second)\n",
    "        time_difference = (finish_time - rounded_time).total_seconds()\n",
    "\n",
    "        remainder = time_difference % float(self.time_interval)\n",
    "\n",
    "        if self.remove_time_in_group <= remainder:\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            return remainder\n",
    "\n",
    "    # Rounding time to nearest time which can divided by time interval\n",
    "    def nearest_time_rounding(self, time):\n",
    "        seconds = time.second\n",
    "        # For example, find nearest value in 0, 10, 20, 30, 40, 50sec\n",
    "        time_points = [time_point for time_point in range(0,60, self.time_interval)]\n",
    "        nearest = min(time_points, key=lambda time_point: abs(time_point - seconds))\n",
    "        \n",
    "        if nearest == time_points[-1] and seconds >= (time_points[-1] + self.remove_time_in_group):\n",
    "            rounded_time = time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n",
    "        else:\n",
    "            rounded_time = time.replace(second=nearest, microsecond=0)\n",
    "\n",
    "        return rounded_time\n",
    "\n",
    "    # Make the same end time\n",
    "    def align_end_time(self, dataframe_1, dataframe_2):\n",
    "        if dataframe_1.index[-1] > dataframe_2.index[-1]:\n",
    "            dataframe_1 = dataframe_1[dataframe_1.index <= dataframe_2.index[-1]]\n",
    "\n",
    "        elif dataframe_1.index[-1] < dataframe_2.index[-1]:\n",
    "            dataframe_2 = dataframe_2[dataframe_2.index <= dataframe_1.index[-1]]\n",
    "\n",
    "        else: \n",
    "            pass # if two dataframe's endtime is same\n",
    "\n",
    "        return dataframe_1, dataframe_2\n",
    "\n",
    "    # Adjust start time and end time processing\n",
    "    # start time processing -> process_type : 0 , finish time processing -> process_type : -1\n",
    "    # start time processing -> process_start_time_trash_sec func , finish time processing -> process_finish_time_trash_sec func    \n",
    "    def adjust_time_index(self, process_type, dataframe, func):\n",
    "        remainder = func(dataframe.index[process_type])\n",
    "        \n",
    "        # the last data only shows one original data, so processing this problem\n",
    "        one_sec = timedelta(seconds=1)\n",
    "\n",
    "        if remainder == False:\n",
    "            # change time to nearest (Start time processing)\n",
    "            if process_type == 0 :\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type])\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "            \n",
    "            # change time to nearest (Finish time processing)\n",
    "            else:\n",
    "                time = self.nearest_time_rounding(dataframe.index[process_type]) - one_sec\n",
    "                new_index = dataframe.index.tolist()\n",
    "                new_index[process_type] = time\n",
    "                dataframe.index = new_index\n",
    "        \n",
    "        # if remainder is under remove_time_in_group, just remove\n",
    "        else:\n",
    "            cutting_time = timedelta(seconds=remainder)\n",
    "            # Start time processing\n",
    "            if process_type == 0:\n",
    "                dataframe = dataframe[dataframe.index >= dataframe.index[process_type] + cutting_time]\n",
    "            \n",
    "            # Finish time processing\n",
    "            # make the seconds like 9, 19, 29...\n",
    "            else:\n",
    "                dataframe = dataframe[dataframe.index <= dataframe.index[-1] - cutting_time - one_sec]\n",
    "\n",
    "        return dataframe\n",
    "    \n",
    "    # Removing error values in group (brain waves and attention score)\n",
    "    def check_invalid_values(self, group):\n",
    "        # find error data length in brain wave\n",
    "        alpha_invalid_series = group['α_wave_raw_data'].diff().eq(0)\n",
    "        alpha_invalid_timestamps = group.index[alpha_invalid_series].tolist()\n",
    "\n",
    "        # find error data length in attention_raw_data\n",
    "        attention_invalid_series = group['attention_raw_data'] == 0\n",
    "        attention_invalid_timestamps = group.index[attention_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        alpha_invalid = has_long_invalid_duration(alpha_invalid_timestamps)\n",
    "        attention_invalid = has_long_invalid_duration(attention_invalid_timestamps)\n",
    "        \n",
    "        # make error values to missing values\n",
    "        if alpha_invalid or attention_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            valid_conditions = (\n",
    "                (group['α_wave_raw_data'].diff() != 0) & \n",
    "                (group['β_wave_raw_data'].diff() != 0) & \n",
    "                (group['θ_wave_raw_data'].diff() != 0) & \n",
    "                (group['δ_wave_raw_data'].diff() != 0) & \n",
    "                (group['γ_wave_raw_data'].diff() != 0) & \n",
    "                (group['attention_raw_data'] != 0)\n",
    "            )\n",
    "            return group[valid_conditions].mean()\n",
    "\n",
    "    # Removing error values in group (hr)\n",
    "    def check_invalid_values_other(self, group):\n",
    "        # find error data length in hr\n",
    "        hr_invalid_series = group['hr_raw_data'] == 0\n",
    "        hr_invalid_timestamps = group.index[hr_invalid_series].tolist()\n",
    "\n",
    "        # check whether the length of error data is over remove_time_in_group second\n",
    "        def has_long_invalid_duration(invalid_timestamps):\n",
    "            if not invalid_timestamps:\n",
    "                return False\n",
    "            for invalid_time in range(1, len(invalid_timestamps)):\n",
    "                if (invalid_timestamps[invalid_time] - invalid_timestamps[invalid_time-1]).seconds > self.remove_time_in_group:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        hr_invalid = has_long_invalid_duration(hr_invalid_timestamps)\n",
    "\n",
    "        # make error values to missing values\n",
    "        if hr_invalid:\n",
    "            return pd.Series([np.nan] * group.shape[1], index=group.columns)\n",
    "\n",
    "        else:\n",
    "            # calculate average except error value\n",
    "            group = group[(group['hr_raw_data'] != 0)]\n",
    "            return group.mean()\n",
    "    \n",
    "    # Process EEG data\n",
    "    def process_eeg_data(self, experiment_id):\n",
    "        if experiment_id not in self.EEG_report.index:\n",
    "            return None\n",
    "\n",
    "        # all experiments in one df\n",
    "        EEG_report_sample = self.EEG_report.loc[[experiment_id],:]\n",
    "\n",
    "        # one dataframe for one column\n",
    "        cols = ['α_wave_raw_data', 'β_wave_raw_data', 'θ_wave_raw_data', 'δ_wave_raw_data', 'γ_wave_raw_data', 'attention_raw_data', 'hrv_raw_data', 'hr_raw_data', 'coherence_flag_raw_data']\n",
    "        parsed_dfs = [self.parse_raw_data(EEG_report_sample, col) for col in cols]\n",
    "\n",
    "        # calculate two interval second because there's two type of time interval in EEG data\n",
    "        interval_sec = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[0])\n",
    "        interval_sec_other = self.time_difference(EEG_report_sample, 'meditation_start_time', 'meditation_finish_time') / len(parsed_dfs[6])\n",
    "\n",
    "        # make two merged dataframe\n",
    "        merged_df = parsed_dfs[0].join(parsed_dfs[1:6])\n",
    "        merged_df_other = parsed_dfs[6].join(parsed_dfs[7:])\n",
    "\n",
    "        # experiment start time\n",
    "        start_time = datetime.strptime(EEG_report_sample.iloc[0]['meditation_start_time'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # change index to time index based on interval second\n",
    "        interval_sec, interval_sec_other = timedelta(seconds=round(interval_sec,2)), timedelta(seconds=round(interval_sec_other,2))\n",
    "        merged_df['time'] = [start_time + time * interval_sec for time in range(len(merged_df))]\n",
    "        merged_df_other['time'] = [start_time + time * interval_sec_other for time in range(len(merged_df_other))]\n",
    "        merged_df, merged_df_other = merged_df.set_index('time'), merged_df_other.set_index('time')\n",
    "        \n",
    "        # comparing the inital experiment error time\n",
    "        counts = [self.count_initial_same_values(merged_df[col]) for col in cols[:6]] + [self.count_initial_same_values(merged_df_other['hr_raw_data'])]\n",
    "        initial_error_times = [counts[error] * interval_sec.total_seconds() if error != 6 else counts[error] * interval_sec_other.total_seconds() for error in range(7)]\n",
    "        initial_error_time = timedelta(seconds=max(initial_error_times))\n",
    "\n",
    "        # dataset start time\n",
    "        real_start_time = start_time + initial_error_time\n",
    "        merged_df, merged_df_other = merged_df[merged_df.index > real_start_time], merged_df_other[merged_df_other.index > real_start_time]\n",
    "        merged_df.index, merged_df_other.index = merged_df.index.round('S'), merged_df_other.index.round('S')\n",
    "\n",
    "        # make the experiment end time same\n",
    "        merged_df, merged_df_other = self.align_end_time(merged_df, merged_df_other)\n",
    "\n",
    "        # start time process -> i : 0 , finish time process -> i : -1\n",
    "        # start time process -> process_start_time_trash_sec func , finish time process -> process_finish_time_trash_sec func\n",
    "        merged_df = self.adjust_time_index(0, merged_df, self.process_start_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(0, merged_df_other, self.process_start_time_trash_sec)\n",
    "        merged_df = self.adjust_time_index(-1, merged_df, self.process_finish_time_trash_sec)\n",
    "        merged_df_other = self.adjust_time_index(-1, merged_df_other, self.process_finish_time_trash_sec)\n",
    "\n",
    "        # grouping\n",
    "        grouped = merged_df.groupby(merged_df.index.floor(self.time_interval_str))\n",
    "        grouped_other = merged_df_other.groupby(merged_df_other.index.floor(self.time_interval_str))\n",
    "\n",
    "        result = grouped.apply(self.check_invalid_values)\n",
    "        result_other = grouped_other.apply(self.check_invalid_values_other)\n",
    "\n",
    "        # final EEG dataset including β/θ SP ratio\n",
    "        EEG_data_per_time_interval = result.merge(result_other, left_index=True, right_index=True)\n",
    "        EEG_data_per_time_interval['β/θ SP'] = EEG_data_per_time_interval['β_wave_raw_data'] / EEG_data_per_time_interval['θ_wave_raw_data']\n",
    "        \n",
    "        EEG_data_per_time_interval = EEG_data_per_time_interval.rename(columns={\n",
    "            'α_wave_raw_data':'alpha_wave',\n",
    "            'β_wave_raw_data':'beta_wave',\n",
    "            'θ_wave_raw_data':'theta_wave',\n",
    "            'δ_wave_raw_data':'delta_wave',\n",
    "            'γ_wave_raw_data':'gamma_wave',\n",
    "            'attention_raw_data' : 'attention',\n",
    "            'hrv_raw_data' : 'hrv',\n",
    "            'hr_raw_data' : 'hr',\n",
    "            'coherence_flag_raw_data' : 'coherence',\n",
    "            'β/θ SP' : 'SP ratio'\n",
    "        })\n",
    "\n",
    "        return EEG_data_per_time_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "class FitbitProcessor:\n",
    "    '''\n",
    "    time_interval : Unified seconds with EEG dataset (ex.10secs) \n",
    "    BM_sum_minutes\n",
    "    Body Movement feature was meaningless because the experiment was conducted usually while sitting on the chair.\n",
    "    So, created a new body movement feature as accumulated Body Movement value from previous time.\n",
    "    And, the previous time is the BM_sum_minutes variable. (ex. 3 hours)\n",
    "    '''\n",
    "    def __init__(self, folder_path, time_interval, BM_sum_minutes):\n",
    "        self.folder_path = folder_path\n",
    "        self.BM_sum_minutes = BM_sum_minutes\n",
    "        self.BM_sum_minutes_str = f'{BM_sum_minutes}T'\n",
    "        self.time_interval = time_interval\n",
    "        self.time_interval_str = f'{time_interval}S'\n",
    "        folder_patterns = [\n",
    "            \"Active Zone Minutes (AZM)/*\",\n",
    "            \"Sleep Score/*\",\n",
    "            \"Stress Journal/CEDA*\",\n",
    "            \"Temperature/Wrist Temperature - *\"\n",
    "        ]\n",
    "        self.things_path = [glob.glob(f\"{folder_path}/{pattern}\") for pattern in folder_patterns]\n",
    "        self.things_path = [item for sublist in self.things_path for item in sublist]\n",
    "        self.wt_count = len(glob.glob(f\"{folder_path}/Temperature/Wrist Temperature - *\"))\n",
    "        self.azm_count = len(glob.glob(f\"{folder_path}/Active Zone Minutes (AZM)/*\"))\n",
    "        self.sleep_count = len(glob.glob(f\"{folder_path}/Sleep Score/*\"))\n",
    "        self.eda_count = len(glob.glob(f\"{folder_path}/Stress Journal/CEDA*\"))\n",
    "        # original fitbit dataset's time interval is 1 min\n",
    "        self.original_interval = 60\n",
    "        self.num_timestamps = self.original_interval // self.time_interval\n",
    "        self.half_point = self.num_timestamps // 2\n",
    "    \n",
    "    def read_filtered_csv(self, path, columns):\n",
    "        name = pd.read_csv(path)\n",
    "        name = name[columns]\n",
    "        name[columns[0]] = pd.to_datetime(name[columns[0]])\n",
    "        return name\n",
    "    \n",
    "    def round_seconds(self, obj):\n",
    "        if obj.second % self.time_interval == 0:\n",
    "            return obj\n",
    "        else:\n",
    "            return obj - timedelta(seconds=obj.second % self.time_interval)\n",
    "    \n",
    "    def round_zero(self, datetime_obj):\n",
    "        datetime_obj = datetime_obj.replace(second=0)\n",
    "        return datetime_obj\n",
    "    \n",
    "    # Process whole fitbit data\n",
    "    def process_fitbit_data(self):\n",
    "        AZM_col = ['date_time', 'total_minutes']\n",
    "        sleep_col = ['timestamp', 'deep_sleep_in_minutes']\n",
    "        stress_col = ['timestamp', 'eda_level_real']\n",
    "        temp_col = ['recorded_time', 'temperature']\n",
    "        \n",
    "        # merge all features\n",
    "        things_col = [AZM_col] * self.azm_count + [sleep_col] * self.sleep_count + [stress_col] * self.eda_count + [temp_col] * self.wt_count\n",
    "        things = [self.read_filtered_csv(path, col) for path, col in zip(self.things_path, things_col)]\n",
    "\n",
    "        # if there's no wrist temperature\n",
    "        if self.wt_count == 0:\n",
    "            # if there's no eda data\n",
    "            # there was no eda in two subjects' fitbit data\n",
    "            if self.eda_count == 0:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep])\n",
    "                \n",
    "                # make final dataframe\n",
    "                df = self.create_final_df([azm, sleep], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                df = df.assign(eda=np.nan)\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            else:\n",
    "                azm = self.process_azm(things[:self.azm_count])\n",
    "                sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "                eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "                Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda])\n",
    "\n",
    "                df = self.create_final_df([azm, sleep, eda], Min_Time, Max_Time)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                df = df.assign(temperature=np.nan)\n",
    "                return df\n",
    "            \n",
    "        # if there's no Active Zone Minutes data\n",
    "        elif self.azm_count == 0:\n",
    "            sleep = self.process_sleep(things[self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(BM=np.nan)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # if there's no sleep data\n",
    "        elif self.sleep_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(sleep=np.nan)\n",
    "\n",
    "            return df            \n",
    "        \n",
    "        # if there's no eda data\n",
    "        elif self.eda_count == 0:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.assign(eda=np.nan)\n",
    "\n",
    "            return df\n",
    "            \n",
    "        # if there's no error in data file\n",
    "        else:\n",
    "            azm = self.process_azm(things[:self.azm_count])\n",
    "            sleep = self.process_sleep(things[self.azm_count:self.azm_count+self.sleep_count])\n",
    "            eda = self.process_eda(things[self.azm_count+self.sleep_count:self.azm_count+self.sleep_count+self.eda_count])\n",
    "            temp = self.process_temperature(things[self.azm_count+self.sleep_count+self.eda_count:])\n",
    "            Min_Time, Max_Time = self.find_time_bounds([azm, sleep, eda, temp])\n",
    "\n",
    "            df = self.create_final_df([azm, sleep, eda, temp], Min_Time, Max_Time)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "\n",
    "            return df\n",
    "    \n",
    "    # Process Active Zone Minutes data\n",
    "    def process_azm(self, azm):\n",
    "\n",
    "        azm = pd.concat(azm, axis=0)\n",
    "        azm = azm.rename(columns={'date_time':'timestamp', 'total_minutes':'BM'})\n",
    "        time_list = []\n",
    "        body_movement = []\n",
    "\n",
    "        for time in azm['timestamp']:\n",
    "            start_timestamp = time - timedelta(seconds=(self.half_point * self.time_interval))\n",
    "\n",
    "            for number in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (number*self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "        for bm in azm['BM']:\n",
    "            for _ in range(self.num_timestamps):\n",
    "                body_movement.append(bm)\n",
    "\n",
    "        azm_list = {'timestamp': time_list, 'BM': body_movement}\n",
    "        azm_final = pd.DataFrame(azm_list)\n",
    "\n",
    "        azm_final['timestamp'] = pd.to_datetime(azm_final['timestamp'])\n",
    "        azm_final.set_index('timestamp', inplace=True)\n",
    "        azm_final = azm_final.resample(self.time_interval_str).asfreq().fillna(0)\n",
    "        azm_final['new_BM'] = azm_final['BM'].rolling(self.BM_sum_minutes_str, closed='right').sum()\n",
    "        azm_final = azm_final.drop(['BM'], axis=1)\n",
    "        azm_final = azm_final.rename(columns={'new_BM':'BM'})\n",
    "        azm_final = azm_final.astype({'BM':'int'})\n",
    "        azm_final.reset_index(inplace=True)\n",
    "        \n",
    "        return azm_final\n",
    "    \n",
    "    # Process sleep data (Deep sleep in minutes)\n",
    "    def process_sleep(self, sleep):\n",
    "        sleep = pd.concat(sleep, axis=0)\n",
    "        sleep['timestamp'] = [\n",
    "                self.round_zero(datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S')) for time in sleep['timestamp']\n",
    "            ]\n",
    "            \n",
    "        sleep = sleep.rename(columns={'deep_sleep_in_minutes':'sleep'})\n",
    "        return sleep\n",
    "    \n",
    "    # Process eda data\n",
    "    def process_eda(self, eda):\n",
    "        eda = pd.concat(eda, axis=0)\n",
    "        eda['timestamp'] = [\n",
    "            self.round_seconds(\n",
    "                datetime.strptime(str(time)[0:19], '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=4)\n",
    "            ) for time in eda['timestamp']]\n",
    "\n",
    "        # EDA Bilinear Interpolation\n",
    "        time_list = []\n",
    "        eda_list = []\n",
    "\n",
    "        for time in range(1, len(eda['timestamp']) - 1):\n",
    "            start_timestamp = eda.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "\n",
    "            for num in range(self.num_timestamps):\n",
    "                new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                time_list.append(new_timestamp)\n",
    "\n",
    "                if num < self.half_point:\n",
    "                    weight = (self.half_point - num) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] - ((eda.iloc[time, 1] - eda.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                elif num == self.half_point:\n",
    "                    value = eda.iloc[time,1]\n",
    "\n",
    "                else:\n",
    "                    weight = (num - self.half_point) / self.num_timestamps\n",
    "                    value = eda.iloc[time, 1] + ((eda.iloc[time + 1, 1] - eda.iloc[time, 1]) * weight)\n",
    "\n",
    "                eda_list.append(round(value, 2))\n",
    "\n",
    "        eda_list = {'timestamp': time_list, 'eda': eda_list}\n",
    "        eda_final = pd.DataFrame(eda_list)\n",
    "\n",
    "        return eda_final\n",
    "    \n",
    "    # Process temperature data\n",
    "    def process_temperature(self, temperature):\n",
    "        #Temperature\n",
    "        temp = pd.concat(temperature, axis=0)\n",
    "        temp = temp.rename(columns={'recorded_time':'timestamp'})\n",
    "\n",
    "        # Temperature Bilinear Interpolation\n",
    "        time_list = []\n",
    "        temp_list = []\n",
    "\n",
    "        for time in range(1, len(temp['timestamp']) - 1):\n",
    "            if self.half_point % 2 != 0:\n",
    "                start_timestamp = temp.iloc[time,0] - timedelta(seconds = self.half_point * self.time_interval)\n",
    "                for num in range(self.num_timestamps):\n",
    "                    new_timestamp = start_timestamp + timedelta(seconds = (num * self.time_interval))\n",
    "                    time_list.append(new_timestamp)\n",
    "\n",
    "                    if num < self.half_point:\n",
    "                        weight = (self.half_point - num) / self.num_timestamps \n",
    "                        value = temp.iloc[time, 1] - ((temp.iloc[time, 1] - temp.iloc[time - 1, 1]) * weight)\n",
    "\n",
    "                    elif num == self.half_point:\n",
    "                        value = temp.iloc[time,1]\n",
    "\n",
    "                    else:\n",
    "                        weight = (num - self.half_point) / self.num_timestamps\n",
    "                        value = temp.iloc[time, 1] + ((temp.iloc[time + 1, 1] - temp.iloc[time, 1]) * weight)\n",
    "\n",
    "                    temp_list.append(round(value, 6))\n",
    "\n",
    "        temp_list = {'timestamp': time_list, 'temperature': temp_list}\n",
    "        temp_final = pd.DataFrame(temp_list)\n",
    "\n",
    "        return temp_final\n",
    "    \n",
    "    # find minimum and maximum time of whole feature\n",
    "    def find_time_bounds(self, dataframes):\n",
    "        min_times = []\n",
    "        max_times = []\n",
    "        \n",
    "        for df in dataframes:\n",
    "            if not df.empty:\n",
    "                min_times.append(df['timestamp'].min())\n",
    "                max_times.append(df['timestamp'].max())\n",
    "                \n",
    "        if not min_times or not max_times:\n",
    "            Min_Time = pd.Timestamp.now(tz='UTC')\n",
    "            Max_Time = pd.Timestamp.now(tz='UTC')\n",
    "        else:\n",
    "            Min_Time = min(min_times)\n",
    "            Max_Time = max(max_times)\n",
    "\n",
    "        return Min_Time, Max_Time\n",
    "\n",
    "    # create dataframe from Min_time to Max_time\n",
    "    def create_final_df(self, datasets, Min_Time, Max_Time):\n",
    "        fitbit = pd.date_range(start=Min_Time, end=Max_Time, freq=self.time_interval_str, name='timestamp')\n",
    "        fitbit = pd.DataFrame(fitbit)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            fitbit = pd.merge(fitbit, dataset, how='outer', on='timestamp')\n",
    "            \n",
    "        fitbit['BM'] = fitbit['BM'].fillna(0)\n",
    "        fitbit['sleep'] = fitbit['sleep'].fillna(method='ffill')\n",
    "\n",
    "        return fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger(EEGProcessor, FitbitProcessor):\n",
    "    '''\n",
    "    We've tested some hyperparameters, and \"time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180\" have shown the best R-squared score.\n",
    "    \n",
    "    [Tested hyperparameters]\n",
    "    1. Time interval : 10, 12, 15, 20sec & eeg_remove_time_in_group : 7, 9, 12, 16sec (7~80% proportion of time interval)\n",
    "    R-squared score was best when we split the dataset into 10 seconds group.\n",
    "    \n",
    "    2. BM (Body Movement) sum minutes : 1h, 1h 30m, 2h, 2h 30m, 3h\n",
    "    R-squared score was best when we set up the BM (Body Movement) sum minutes as 3 hours.     \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eeg_filepath, fitbit_folderpath, time_interval=10, eeg_remove_time_in_group=7, BM_sum_minutes=180):\n",
    "        # Initialize by calling parent class constructor\n",
    "        EEGProcessor.__init__(self, eeg_filepath, time_interval, eeg_remove_time_in_group)\n",
    "        FitbitProcessor.__init__(self, fitbit_folderpath, time_interval, BM_sum_minutes)\n",
    "        \n",
    "        self.eeg_filepath = eeg_filepath\n",
    "\n",
    "    # Merge EEG and Fitbit data\n",
    "    def merge_data(self):\n",
    "        # processing EEG data\n",
    "        eeg_data = pd.read_csv(self.eeg_filepath)\n",
    "        result_dfs = []\n",
    "        \n",
    "        # For all experiments in the eeg data csv file\n",
    "        for exp_id in range(3, len(eeg_data)):\n",
    "            processed_data = self.process_eeg_data(exp_id)\n",
    "            if processed_data is not None:\n",
    "                result_dfs.append(processed_data)\n",
    "                \n",
    "        if result_dfs:\n",
    "            combined_eeg = pd.concat(result_dfs)\n",
    "            combined_eeg.index = pd.to_datetime(combined_eeg.index)\n",
    "        \n",
    "        # processing Fitbit data\n",
    "        fitbit_data = self.process_fitbit_data()\n",
    "        fitbit_data.index = pd.to_datetime(fitbit_data.index)\n",
    "\n",
    "        # merging two dataframes\n",
    "        if 'combined_eeg' in locals() and not fitbit_data.empty:\n",
    "            merged_df = combined_eeg.merge(fitbit_data, left_index=True, right_index=True, how='left')\n",
    "            return merged_df\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01937409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_jm.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_JM\")\n",
    "\n",
    "final_jm = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_yh.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_YH\")\n",
    "\n",
    "final_yh = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SJ\")\n",
    "\n",
    "\n",
    "final_sj = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_sa.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_SA\")\n",
    "\n",
    "\n",
    "final_sa = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_bs.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_BS\")\n",
    "\n",
    "\n",
    "final_bs = merger.merge_data()\n",
    "\n",
    "# eeg_filepath, fitbit_folderpath, time_interval, eeg_remove_time_in_group\n",
    "merger = DataMerger(r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\EEG_mj.csv\",\n",
    "                    r\"C:\\Users\\ballj\\OneDrive\\바탕 화면\\Fitbit_MJ\")\n",
    "\n",
    "\n",
    "final_mj = merger.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e258c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([final_jm, final_sj, final_bs, final_yh, final_mj, final_sa])\n",
    "dataset = dataset.sort_index()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032128a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Temperature Imputation (Random Forest Imputation)\n",
    "temperature_na1 = dataset_original[dataset_original['temperature'].isna() & dataset_original.drop(columns='temperature').notna().all(axis=1)]\n",
    "temperature_na1 = temperature_na1.reset_index()\n",
    "temperature_na1 = temperature_na1.drop(['index'], axis=1)\n",
    "\n",
    "full_dataset1 = dataset_original.dropna()\n",
    "full_dataset1 = full_dataset1.reset_index()\n",
    "full_dataset1 = full_dataset1.drop(['index'], axis=1)\n",
    "\n",
    "dataset_temp = pd.concat([full_dataset1, temperature_na1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e759a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using Random Forest\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Split the dataset into two parts: with and without missing 'temperature' values\n",
    "dataset_with_temperature = dataset_temp.dropna(subset=['temperature'])\n",
    "dataset_without_temperature = dataset_temp[dataset_temp['temperature'].isnull()]\n",
    "\n",
    "# Train the model on the rows with no missing temperature values\n",
    "regressor.fit(dataset_with_temperature.drop('temperature', axis=1), dataset_with_temperature['temperature'])\n",
    "\n",
    "# Predict the missing temperature values\n",
    "predicted_temperatures = regressor.predict(dataset_without_temperature.drop('temperature', axis=1))\n",
    "\n",
    "# Fill in the missing values in the original dataframe\n",
    "dataset_temp.loc[dataset_temp['temperature'].isnull(), 'temperature'] = predicted_temperatures\n",
    "\n",
    "dataset_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784441d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "eda_na = dataset_original[dataset_original['eda'].isna() & dataset_original.drop(columns='eda').notna().all(axis=1)]\n",
    "eda_na = eda_na.reset_index()\n",
    "eda_na = eda_na.drop(['index'], axis=1)\n",
    "\n",
    "dataset_eda = pd.concat([full_dataset1, eda_na])\n",
    "\n",
    "# Impute missing values using Random Forest\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Split the dataset into two parts: with and without missing 'temperature' values\n",
    "dataset_with_eda = dataset_eda.dropna(subset=['eda'])\n",
    "dataset_without_eda = dataset_eda[dataset_eda['eda'].isnull()]\n",
    "\n",
    "# Train the model on the rows with no missing temperature values\n",
    "regressor.fit(dataset_with_eda.drop('eda', axis=1), dataset_with_eda['eda'])\n",
    "\n",
    "# Predict the missing temperature values\n",
    "predicted_edas = regressor.predict(dataset_without_eda.drop('eda', axis=1))\n",
    "\n",
    "# Fill in the missing values in the original dataframe\n",
    "dataset_eda.loc[dataset_eda['eda'].isnull(), 'eda'] = predicted_edas\n",
    "dataset_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([dataset_temp, dataset_eda])\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c270ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "attention_original = merged_df['attention'].copy()\n",
    "coherence_original = merged_df['coherence'].copy()\n",
    "# Temp - Standard Scaler & Else - Min-Max Scaling\n",
    "features_to_minmax = merged_df.drop(['temperature', 'SP ratio', 'coherence', 'attention'], axis=1)\n",
    "feature_to_standardize = merged_df[['temperature']]\n",
    "feature_to_minmax_0_1 = merged_df[['SP ratio']]\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler_0_1 = MinMaxScaler(feature_range=(0, 1))\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "features_to_minmax_scaled = minmax_scaler.fit_transform(features_to_minmax)\n",
    "feature_to_standardize_scaled = standard_scaler.fit_transform(feature_to_standardize)\n",
    "feature_to_minmax_0_1_scaled = minmax_scaler_0_1.fit_transform(feature_to_minmax_0_1)\n",
    "\n",
    "# convert to dataframe\n",
    "features_to_minmax_scaled_df = pd.DataFrame(features_to_minmax_scaled, \n",
    "                                            index=merged_df.index, \n",
    "                                            columns=features_to_minmax.columns)\n",
    "feature_to_standardize_scaled_df = pd.DataFrame(feature_to_standardize_scaled, \n",
    "                                                index=merged_df.index, \n",
    "                                                columns=['temperature'])\n",
    "feature_to_minmax_0_1_scaled_df = pd.DataFrame(feature_to_minmax_0_1_scaled, \n",
    "                                                index=merged_df.index, \n",
    "                                                columns=['SP ratio'])\n",
    "\n",
    "merged_df.update(features_to_minmax_scaled_df)\n",
    "merged_df.update(feature_to_standardize_scaled_df)\n",
    "merged_df.update(feature_to_minmax_0_1_scaled_df)\n",
    "\n",
    "merged_df['coherence'] = coherence_original\n",
    "merged_df['attention'] = attention_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aac158",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c13764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df['attention'].min())\n",
    "print(merged_df['attention'].max())\n",
    "print(merged_df['SP ratio'].min())\n",
    "print(merged_df['SP ratio'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd672861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(merged_df['attention'].min(), merged_df['attention'].max()))\n",
    "merged_df['SP_ratio_scaled'] = scaler.fit_transform(merged_df[['SP ratio']])\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(['SP ratio'], axis=1)\n",
    "merged_df = merged_df.rename(columns={'SP_ratio_scaled':'SP ratio'})\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a753334",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(r'C:\\Users\\ballj\\OneDrive\\바탕 화면\\Final_dataset_after_postprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439e928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86020010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
